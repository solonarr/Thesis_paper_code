{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11835190,"sourceType":"datasetVersion","datasetId":7435499},{"sourceId":11870249,"sourceType":"datasetVersion","datasetId":7459568},{"sourceId":11873034,"sourceType":"datasetVersion","datasetId":7461576},{"sourceId":11875148,"sourceType":"datasetVersion","datasetId":7463085},{"sourceId":11876296,"sourceType":"datasetVersion","datasetId":7463895},{"sourceId":11888350,"sourceType":"datasetVersion","datasetId":7472215},{"sourceId":11889813,"sourceType":"datasetVersion","datasetId":7473183},{"sourceId":11918512,"sourceType":"datasetVersion","datasetId":7492757}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pingouin","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Normalization ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfeature_matrix_pt = pd.read_csv(\"/kaggle/input/raw-data/output_with_params_pt.csv\")\nfeature_matrix_pr = pd.read_csv(\"/kaggle/input/raw-data/output_with_params_pr.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_matrix_pr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Создаем столбец с длиной текста в словах\ndef get_text_length(text):\n    if isinstance(text, str):\n        return len(text.split())\n    else:\n        return 0  # Обработка NaN и нестроковых значений\n\n# Применяем функцию к столбцу с текстом\nfeature_matrix_pt['text_length'] = feature_matrix_pt['Текст открытки'].apply(get_text_length)\nfeature_matrix_pr['text_length'] = feature_matrix_pr['text'].apply(get_text_length)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_matrix_pt.drop(\n    feature_matrix_pt[feature_matrix_pt['text_length'] <= 2].index,\n    inplace=True\n)\nfeature_matrix_pt.reset_index(drop=True, inplace=True)\n\n\nfeature_matrix_pr.drop(\n    feature_matrix_pr[feature_matrix_pr['text_length'] <= 2].index,\n    inplace=True\n)\nfeature_matrix_pr.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Определим, какие колонки являются признаками \nmeta_columns = ['text', 'year', 'decade', 'text_length']\nfeature_columns = [col for col in feature_matrix_pr.columns if col not in meta_columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Признаки, которые НЕ нужно нормализовать (уже индексные, средние и т.п.)\npotentially_problematic_columns = [\n    'noun_abstr_index_abs',\n    'adj_abstr_index_abs',\n    'flesch_kincaid_index_abs',\n    'avg_vp_length_abs',\n    'avg_np_length_abs',\n    'max_tree_depth_abs',\n    'word length_abs',\n    'type-token ratio_abs',\n    'mean_sentence_length_abs'\n]\n\n# Копируем матрицу признаков\nnormalized_feature_matrix_pt = feature_matrix_pt.copy()\n\n# Удаляем строки с подозрительно высокими частотами (только для частотных признаков)\nrows_to_keep = []\nfor index, row in feature_matrix_pt.iterrows():\n    keep_row = True\n    for feature in feature_columns:\n        if feature not in potentially_problematic_columns:\n            try:\n                dif = row[feature] / (row['text_length'] + 1)\n                if dif > 1:\n                    keep_row = False\n                    break\n            except ZeroDivisionError:\n                keep_row = False\n                break\n    if keep_row:\n        rows_to_keep.append(index)\n\n# Оставляем только нормальные строки\nnormalized_feature_matrix_pt = normalized_feature_matrix_pt.iloc[rows_to_keep]\n\n# Применяем нормализацию по Байберу ТОЛЬКО к частотным признакам\nfor feature in feature_columns:\n    if feature not in potentially_problematic_columns:\n        normalized_feature_matrix_pt[feature] = (\n            normalized_feature_matrix_pt[feature] / (normalized_feature_matrix_pt['text_length'] + 1) * 100\n        ).replace([np.inf, -np.inf], np.nan).round(3)\n    else:\n        # Оставляем как есть\n        normalized_feature_matrix_pt[feature] = normalized_feature_matrix_pt[feature]\n\n# Сброс индекса\nnormalized_feature_matrix_pt = normalized_feature_matrix_pt.reset_index(drop=True)\n\n# Готово\nnormalized_feature_matrix_pt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Признаки, которые НЕ нужно нормализовать (уже индексные, средние и т.п.)\npotentially_problematic_columns = [\n    'noun_abstr_index_abs',\n    'adj_abstr_index_abs',\n    'flesch_kincaid_index_abs',\n    'avg_vp_length_abs',\n    'avg_np_length_abs',\n    'max_tree_depth_abs',\n    'word length_abs',\n    'type-token ratio_abs',\n    'mean_sentence_length_abs'\n]\n\n# Копируем матрицу признаков\nnormalized_feature_matrix_pr = feature_matrix_pr.copy()\n\n# Удаляем строки с подозрительно высокими частотами (только для частотных признаков)\nrows_to_keep = []\nfor index, row in feature_matrix_pr.iterrows():\n    keep_row = True\n    for feature in feature_columns:\n        if feature not in potentially_problematic_columns:\n            try:\n                dif = row[feature] / (row['text_length'] + 1)\n                if dif > 1:\n                    keep_row = False\n                    break\n            except ZeroDivisionError:\n                keep_row = False\n                break\n    if keep_row:\n        rows_to_keep.append(index)\n\n# Оставляем только нормальные строки\nnormalized_feature_matrix_pr = normalized_feature_matrix_pr.iloc[rows_to_keep]\n\n# Применяем нормализацию по Байберу ТОЛЬКО к частотным признакам\nfor feature in feature_columns:\n    if feature not in potentially_problematic_columns:\n        normalized_feature_matrix_pr[feature] = (\n            normalized_feature_matrix_pr[feature] / (normalized_feature_matrix_pr['text_length'] + 1) * 100\n        ).replace([np.inf, -np.inf], np.nan).round(3)\n    else:\n        # Оставляем как есть\n        normalized_feature_matrix_pr[feature] = normalized_feature_matrix_pr[feature]\n\n# Сброс индекса\nnormalized_feature_matrix_pr = normalized_feature_matrix_pr.reset_index(drop=True)\n\n# Готово\nnormalized_feature_matrix_pr\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normalized_feature_matrix_pr.to_csv('normalized_feature_matrix_pt.csv')\nnormalized_feature_matrix_pr.to_csv('normalized_feature_matrix_pr.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_columns = [col for col in normalized_feature_matrix_pr.columns if col not in ['Текст открытки', 'decade', 'год', 'text_length', 'Unnamed: 0', 'year', 'text', 'other_coordination_abs', 'type']]\nlen(feature_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in feature_columns:\n        normalized_feature_matrix_pr[col] = normalized_feature_matrix_pr[col].fillna(0).astype('float64')\n\nnan_cols = normalized_feature_matrix_pr.columns[normalized_feature_matrix_pr.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\nfor col in feature_columns:\n        normalized_feature_matrix_pr[col] = normalized_feature_matrix_pr[col].fillna(0).astype('float64')\n\nnan_cols = normalized_feature_matrix_pr.columns[normalized_feature_matrix_pr.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import ttest_ind\nfrom statsmodels.stats.multitest import multipletests\nimport pingouin as pg\n\ndef analyze_large_corpora(corpus1_df, corpus2_df, features, chunk_size=10000):\n    \"\"\"\n    Анализ предзагруженных корпусов\n    \n    Параметры:\n    corpus1_df, corpus2_df - предзагруженные DataFrame\n    features - список признаков для анализа\n    chunk_size - размер чанка для обработки (по умолчанию 10,000)\n    \"\"\"\n    \n    # Инициализация результатов\n    results = {feature: {'mean1': 0.0, 'mean2': 0.0, 'd': 0.0, 'p': 1.0} \n               for feature in features}\n    \n    # Обработка корпуса 1\n    total1 = 0\n    for i in range(0, len(corpus1_df), chunk_size):\n        chunk = corpus1_df.iloc[i:i+chunk_size]\n        total1 += len(chunk)\n        for feature in features:\n            results[feature]['mean1'] += chunk[feature].sum()\n\n    # Обработка корпуса 2\n    total2 = 0\n    for i in range(0, len(corpus2_df), chunk_size):\n        chunk = corpus2_df.iloc[i:i+chunk_size]\n        total2 += len(chunk)\n        for feature in features:\n            results[feature]['mean2'] += chunk[feature].sum()\n\n    # Расчет финальных средних\n    for feature in features:\n        results[feature]['mean1'] /= total1 if total1 > 0 else 1\n        results[feature]['mean2'] /= total2 if total2 > 0 else 1\n\n    # Расчет статистик\n    p_values = []\n    cohens_d_values = []\n    for feature in features:\n        # Данные для признака\n        data1 = corpus1_df[feature].values\n        data2 = corpus2_df[feature].values\n        \n        # t-тест Уэлча\n        try:\n            _, p = ttest_ind(data1, data2, equal_var=False, nan_policy='omit')\n        except:\n            p = 1.0\n        \n        # Cohen's d\n        try:\n            # n1, n2 = len(data1), len(data2)\n            # var1 = np.nanvar(data1, ddof=1)\n            # var2 = np.nanvar(data2, ddof=1)\n            # pooled_var = ((n1-1)*var1 + (n2-1)*var2) / (n1 + n2 - 2)\n            # pooled_std = np.sqrt(pooled_var) if pooled_var > 0 else 0\n            # d = (np.nanmean(data1) - np.nanmean(data2)) / pooled_std if pooled_std != 0 else 0\n            d = pg.compute_effsize(data1, data2, eftype='cohen')\n        except:\n            d = 0\n        \n        p_values.append(p)\n        cohens_d_values.append(d)\n\n    # Поправка FDR\n    p_values = [1.0 if np.isnan(p) else p for p in p_values]\n    _, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n\n    # Сбор результатов\n    return pd.DataFrame({\n        'feature': features,\n        'mean_corpus1': [results[f]['mean1'] for f in features],\n        'mean_corpus2': [results[f]['mean2'] for f in features],\n        'p_value': p_values,\n        'p_corrected': p_corrected,\n        'cohens_d': cohens_d_values\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Анализ\nresults = analyze_large_corpora(normalized_feature_matrix_pt, normalized_feature_matrix_pr, feature_columns)\nresults","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Фильтрация значимых результатов\nsignificant = results[(results['p_corrected'] < 0.05) & (abs(results['cohens_d']) >= 0.5)]\nsignificant","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normalized_feature_matrix_pr['type'] = 'diary'\nnormalized_feature_matrix_pt['type'] = 'postcard'\n\nnormalized_feature_matrix_pt.rename(columns={\"Текст открытки\": \"text\", \"Год открытки\": \"year\"}, inplace=True)\ncols = normalized_feature_matrix_pr.columns.tolist()\ncols[0], cols[1] = cols[1], cols[0]\nnormalized_feature_matrix_pr = normalized_feature_matrix_pr[cols]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normalized_feature_matrix_pt.columns == normalized_feature_matrix_pr.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def concatenate_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n    \n    if df1.columns.tolist() != df2.columns.tolist():\n        print(\"Ошибка: DataFrame имеют разные колонки.\")\n        return None\n    \n    concatenated_df = pd.concat([df1, df2], ignore_index=True)\n    return concatenated_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df = concatenate_dataframes(normalized_feature_matrix_pt, normalized_feature_matrix_pr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# United factor analysis ","metadata":{}},{"cell_type":"code","source":"counts = result_df.groupby(['decade', 'type']).size().unstack(fill_value=0)\nprint(\"Исходное распределение:\\n\", counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_count = (\n    result_df.groupby(['decade', 'type']).size().min()\n)\n\n# Найдём только те декады, где есть достаточно и того, и другого\nvalid_decades = result_df.groupby(['decade', 'type']).size().unstack().dropna()\nvalid_decades = valid_decades[(valid_decades['diary'] >= min_count) & (valid_decades['postcard'] >= min_count)].index\n\n# Теперь выборка\nbalanced_samples = []\n\nfor decade in valid_decades:\n    for text_type in ['postcard', 'diary']:\n        subset = result_df[(result_df['decade'] == decade) & (result_df['type'] == text_type)]\n        sampled = subset.sample(n=min_count, random_state=42)\n        balanced_samples.append(sampled)\n\ndf_balanced = pd.concat(balanced_samples, ignore_index=True)\n\nprint(\"\\nРаспределение после выравнивания:\\n\", \n      df_balanced.groupby(['decade', 'type']).size().unstack())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_balanced","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade scipy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pingouin as pg\n\nresult_num = result_df.select_dtypes(include='number')\nbalanced_num = df_balanced.select_dtypes(include='number')\n\n\n# multivariate t-test для двух наборов данных\ntest = pg.multivariate_ttest(result_num, balanced_num)\n\nprint(test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from scipy.stats import ttest_ind, levene, ks_2samp\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # Убедимся, что только числовые признаки\n# numeric_cols = result_df.select_dtypes(include=[np.number]).columns.intersection(\n#     df_balanced.select_dtypes(include=[np.number]).columns\n# )\n\n# # Для хранения результатов\n# results = []\n\n# for col in numeric_cols:\n#     # t-test\n#     t_stat, t_p = ttest_ind(result_df[col], df_balanced[col], equal_var=False, nan_policy='omit')\n    \n#     # Levene's test (на гомогенность дисперсий)\n#     lev_stat, lev_p = levene(result_df[col], df_balanced[col], center='median')\n    \n#     # K-S test (на одинаковость распределений)\n#     ks_stat, ks_p = ks_2samp(result_df[col].dropna(), df_balanced[col].dropna())\n    \n#     results.append({\n#         'feature': col,\n#         't_pvalue': t_p,\n#         'levene_pvalue': lev_p,\n#         'ks_pvalue': ks_p\n#     })\n\n# # В DataFrame\n# results_df = pd.DataFrame(results)\n\n# # Сколько признаков НЕ имеют значимых различий (p > 0.05)\n# no_diff_t = (results_df['t_pvalue'] > 0.05).sum()\n# no_diff_ks = (results_df['ks_pvalue'] > 0.05).sum()\n\n# print(f\"Без значимых различий по t-тесту: {no_diff_t}/{len(numeric_cols)} признаков\")\n# print(f\"Без значимых различий по K-S тесту: {no_diff_ks}/{len(numeric_cols)} признаков\")\n\n# # Показать признаки с наиболее сильными различиями\n# print(results_df.nsmallest(10, 'ks_pvalue'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Можно выбрать случайные 5 признаков для графиков\n\n# sample_cols = np.random.choice(numeric_cols, size=5, replace=False)\n\n# for col in sample_cols:\n#     plt.figure(figsize=(8, 4))\n#     sns.kdeplot(result_df[col], label='Original', fill=True, alpha=0.5)\n#     sns.kdeplot(df_balanced[col], label='Balanced', fill=True, alpha=0.5)\n#     plt.title(f\"Распределение признака: {col}\")\n#     plt.legend()\n#     plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"descriptive_stats = df_balanced.describe().T[['mean', 'min', 'max', 'std']]\ndescriptive_stats['range'] = descriptive_stats['max'] - descriptive_stats['min']\n\n# Сохраняем в таблицу\ndescriptive_stats.to_csv('descriptive_stats.csv')\nprint(descriptive_stats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Сначала создаем копию normalized_feature_matrix:\nfa_matrix = df_balanced.copy(deep=True)\n\nfa_matrix = fa_matrix.drop(['year', 'text_length'], axis=1)\n\nfa_matrix.head()\n\n\nfull_fa_matrix = result_df.copy(deep=True)\nfull_fa_matrix = full_fa_matrix.drop(['year', 'text_length'], axis=1)\n\nfa_matrix.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Выбираем только числовые признаки (исключаем метаданные)\nfeature_columns = [col for col in fa_matrix.columns if col not in ['text', 'decade', 'type']]\n\n# Создаем DataFrame только с признаками для факторного анализа\nanalysis_df = fa_matrix[feature_columns].copy(deep=True)\nanalysis_df_full = full_fa_matrix[feature_columns].copy(deep=True)\n\n# Убедимся, что все данные числовые\nprint(analysis_df.dtypes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install factor_analyzer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nan_cols = analysis_df.columns[analysis_df.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\ndf_with_nan = analysis_df[nan_cols]\n\n# 3. Создаем булеву маску, где True соответствует строкам, содержащим хотя бы один NaN\nrows_with_nan = df_with_nan.isnull().any(axis=1)\n\n# 4. Считаем количество строк, содержащих хотя бы один NaN\nprint(rows_with_nan.sum())\n\nfor col in nan_cols:\n    analysis_df[col] = analysis_df[col].fillna(0).astype('float64')\n\n\nprint(analysis_df.isnull().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nan_cols = analysis_df_full.columns[analysis_df_full.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\ndf_with_nan = analysis_df_full[nan_cols]\n\n# 3. Создаем булеву маску, где True соответствует строкам, содержащим хотя бы один NaN\nrows_with_nan = df_with_nan.isnull().any(axis=1)\n\n# 4. Считаем количество строк, содержащих хотя бы один NaN\nprint(rows_with_nan.sum())\n\nfor col in nan_cols:\n    analysis_df_full[col] = analysis_df_full[col].fillna(0).astype('float64')\n\n\nprint(analysis_df_full.isnull().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stddevs = analysis_df.std()\nprint(stddevs[stddevs == 0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"analysis_df = analysis_df.drop('other_coordination_abs', axis=1)\nstddevs = analysis_df.std()\nprint(stddevs[stddevs == 0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stddevs = analysis_df_full.std()\nprint(stddevs[stddevs == 0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"analysis_df_full = analysis_df_full.drop('other_coordination_abs', axis=1)\nstddevs = analysis_df_full.std()\nprint(stddevs[stddevs == 0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_columns_new = [col for col in analysis_df.columns]\nlen(feature_columns_new)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from factor_analyzer import FactorAnalyzer\n\n# Инициализация PFA без вращения\nfa = FactorAnalyzer(n_factors=len(feature_columns), rotation=None, method='principal')\n\n# Подгонка модели\nfa.fit(analysis_df[feature_columns_new])\n\n# Собственные значения\nev, _ = fa.get_eigenvalues()\n\nloadings_first = fa.loadings_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from factor_analyzer import FactorAnalyzer\n\n# Инициализация и обучение модели на всех данных\nfa_full = FactorAnalyzer(\n    n_factors=len(feature_columns),\n    rotation=None,\n    method='principal'\n)\n\nfa_full.fit(analysis_df_full[feature_columns_new])  # analysis_df — весь ваш DataFrame с данными\n\n# Получение нагрузок\nloadings_full = fa_full.loadings_  # размерность: (признаки, факторы)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom factor_analyzer import FactorAnalyzer\nfrom tqdm import tqdm\n\n# Исходные данные: df — DataFrame с числовыми переменными для факторного анализа\n\nn_iterations = 10\nsample_fraction = 0.07\nn_factors = len(feature_columns)  # например, нужно 3 фактора\n\n# Хранить факторные нагрузки для каждой переменной и фактора\nall_loadings = []\n\ndef has_zero_variance(df):\n    return any(df.std() == 0)\n\nfor i in tqdm(range(n_iterations)):\n    # Бутстрэп-сэмпл\n    sample = analysis_df.sample(frac=sample_fraction, replace=True, random_state=None)\n\n    if sample.shape[0] <= n_factors or sample.shape[0] <= sample.shape[1]:\n        # Пропустить итерацию, если строк мало\n        continue\n\n    if has_zero_variance(sample[feature_columns_new]):\n        continue\n    \n    # Факторный анализ\n    fa = FactorAnalyzer(n_factors=n_factors, rotation=None, method='principal')\n    fa.fit(sample[feature_columns_new])\n    \n    # Получить факторные нагрузки\n    loadings = fa.loadings_  # shape: (n_features, n_factors)\n    all_loadings.append(loadings)\n\n# Преобразуем в numpy массив: (n_iterations, n_features, n_factors)\nall_loadings = np.array(all_loadings)\n\nn_features = all_loadings.shape[1]\n\n# Для каждой переменной и каждого фактора — доверительный интервал\nfor var_idx in range(n_features):\n    for factor_idx in range(n_factors-1):\n        # Получаем распределение нагрузок\n        load_dist = all_loadings[:, var_idx, factor_idx]\n        \n        # Доверительный интервал 95%\n        lower = np.percentile(load_dist, 2.5)\n        upper = np.percentile(load_dist, 97.5)\n        mean = np.mean(load_dist)\n        \n        print(f\"Переменная {var_idx+1}, Фактор {factor_idx+1}: \"\n              f\"Среднее = {mean:.3f}, 95% ДИ = [{lower:.3f}, {upper:.3f}]\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# График осыпи\nplt.figure(figsize=(10, 6))\nplt.scatter(range(1, len(ev)+1), ev)\nplt.plot(range(1, len(ev)+1), ev)\nplt.title('Scree Plot')\nplt.xlabel('Номер фактора')\nplt.ylabel('Собственное значение')\nplt.axhline(y=1, color='r', linestyle='--')\nplt.show()\n\n# Таблица собственных значений и доли дисперсии\neigenvalues_table = pd.DataFrame({\n    'Factor': range(1, len(ev)+1),\n    'Eigenvalue': ev,\n    '% of Variance': (ev / ev.sum()) * 100,\n    'Cumulative %': (ev.cumsum() / ev.sum()) * 100\n})\n\nprint(eigenvalues_table.head(11))  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Проверка наличия NaN и inf\nprint(\"Количество NaN в данных:\", analysis_df.isna().sum().sum())\nprint(\"Количество inf в данных:\", np.isinf(analysis_df.values).sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Замена NaN и inf\n# analysis_df_clean.fillna(0, inplace=True)\n# analysis_df_clean.replace([np.inf, -np.inf], 0, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Инициализация модели с 9 факторами\nfa_promax = FactorAnalyzer(n_factors=9, rotation='promax', method='principal')\nfa_promax.fit(analysis_df[feature_columns_new])\n\n# Факторные нагрузки\nloadings = pd.DataFrame(\n    fa_promax.loadings_,\n    columns=[f'Factor {i+1}' for i in range(9)],\n    index=feature_columns_new\n)\n\n# Сохраняем нагрузки в CSV\nloadings.to_csv('factor_loadings.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Выводим значимые нагрузки \nsignificant_loadings = loadings.applymap(lambda x: x if abs(x) > 0.3 else None)\n# significant_loadings.dropna(how='all')\n\nfor factor in significant_loadings.columns:\n    print(f'Нагрузки для {factor}:')\n\n    # Берём нагрузки для текущего фактора и исключаем NaN\n    for feature, loading in significant_loadings[factor].items():\n        if not pd.isna(loading):\n            print(f'  - {feature}: {loading}')\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Получаем факторные корреляции\nfactor_corr = pd.DataFrame(\n    fa_promax.phi_,\n    columns=[f'Factor {i+1}' for i in range(9)],\n    index=[f'Factor {i+1}' for i in range(9)]\n)\n\n# Визуализация\nplt.figure(figsize=(10, 8))\nsns.heatmap(factor_corr, annot=True, cmap='gray', vmin=-1, vmax=1)\nplt.title('Межфакторные корреляции (Promax)')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Получаем факторные корреляции\nfactor_corr = pd.DataFrame(\n    fa_promax.phi_,\n    columns=[f'Factor {i+1}' for i in range(9)],\n    index=[f'Factor {i+1}' for i in range(9)]\n)\n\n# Визуализация\nplt.figure(figsize=(10, 8))\nsns.heatmap(factor_corr, annot=True, cmap='gray', vmin=-1, vmax=1)\nplt.title('Межфакторные корреляции (Promax)')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loadings = pd.read_csv('/kaggle/input/united-loading-final/factor_loadings (3).csv', index_col='Unnamed: 0')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Порог значимости для включения признаков\nthreshold = 0.30\n\n# Находим абсолютные нагрузки\nloadings_abs = loadings.abs()\n\n# Индексы факторов с максимальной нагрузкой для каждого признака\nmax_index = loadings_abs.idxmax(axis=1)\n\n# Создаем словарь для хранения признаков по факторам\nfactor_dict = {}\n\n# Заполняем словарь, выбирая только те нагрузки, которые выше порога\nfor feature in loadings.index:\n    factor = max_index[feature]\n    max_loading = loadings_abs.loc[feature, factor]\n\n    if max_loading > threshold:\n        if factor not in factor_dict:\n            factor_dict[factor] = []\n        factor_dict[factor].append([feature, float(loadings.loc[feature, factor])])\n\nfactor_dict\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Порог значимости для включения признаков\nthreshold = 0.30\n\n# Находим абсолютные нагрузки\nloadings_abs = loadings.abs()\n\n# Индексы факторов с максимальной нагрузкой для каждого признака\nmax_index = loadings_abs.idxmax(axis=1)\n\n# Создаем словарь для хранения признаков по факторам\nfactor_dict = {}\n\n# Заполняем словарь, выбирая только те нагрузки, которые выше порога\nfor feature in loadings.index:\n    factor = max_index[feature]\n    max_loading = loadings_abs.loc[feature, factor]\n\n    if max_loading > threshold:\n        if factor not in factor_dict:\n            factor_dict[factor] = []\n        factor_dict[factor].append([feature, float(loadings.loc[feature, factor])])\n\nfactor_dict\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Factor scores","metadata":{}},{"cell_type":"code","source":"nan_cols = df_balanced.columns[df_balanced.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in nan_cols:\n    df_balanced[col] = df_balanced[col].fillna(0).astype('float64')\n\nnan_cols = df_balanced.columns[df_balanced.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\ndf_with_nan = df_balanced[nan_cols]\n\n# Создаем булеву маску, где True соответствует строкам, содержащим хотя бы один NaN\nrows_with_nan = df_with_nan.isnull().any(axis=1)\n\n# Считаем количество строк, содержащих хотя бы один NaN\nprint(rows_with_nan.sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix  = df_balanced.copy()\nstandardized_feature_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix  = standardized_feature_matrix.drop(['text', 'year', 'decade', 'type', 'text_length'], axis=1)\nstandardized_feature_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# descriptive_stats = pd.read_csv('/kaggle/input/pt-descrip-stats/descriptive_stats_pt.csv', index_col='Unnamed: 0')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in standardized_feature_matrix.columns:\n    mean = descriptive_stats.loc[column, 'mean']\n    std = descriptive_stats.loc[column, 'std']\n\n    standardized_feature_matrix[column] = (standardized_feature_matrix[column] - mean) / std\nstandardized_feature_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_columns_new = [col for col in standardized_feature_matrix.columns]\nlen(feature_columns_new)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Среднее после стандартизации должно быть ~0\nprint(standardized_feature_matrix[feature_columns_new].mean().round(2))  \n\n# Стандартное отклонение должно быть ~1\nprint(standardized_feature_matrix[feature_columns_new].std().round(2))   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Создаем DataFrame factor_scores с текстами и декадами\nfactor_scores = df_balanced[['text', 'decade', 'type']].copy()\n\n# 2. Связываем standardized_feature_matrix с factor_scores по индексам\nstandardized_features = standardized_feature_matrix.copy()\nstandardized_features.index = factor_scores.index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Функция для расчета факторной оценки\ndef calculate_factor_score(row, factor_features):\n    score = 0\n    for feature, loading in factor_features:\n        # Получаем стандартизированное значение признака\n        value = row[feature]\n        # Учитываем знак нагрузки: + если loading > 0, - если loading < 0\n        score += value * (1 if loading > 0 else -1)\n    return score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Для каждого фактора из factor_dict вычисляем оценку\nfor factor, features in factor_dict.items():\n    # Признаки и их нагрузки для текущего фактора\n    factor_features = [(feat[0], feat[1]) for feat in features]\n\n    # Проверяем наличие признаков в standardized_features\n    missing = [feat[0] for feat in features if feat[0] not in standardized_features.columns]\n    if missing:\n        print(f\"Предупреждение: Признаки {missing} отсутствуют в standardized_feature_matrix.\")\n        continue\n\n    # Рассчитываем факторную оценку\n    factor_scores[factor] = standardized_features.apply(\n        lambda row: calculate_factor_score(row, factor_features), axis=1\n    )\n\nfactor_scores.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"factor_scores.to_csv('factor_scores.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text by factors","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfactor_scores = pd.read_csv('/kaggle/input/normal-data/factor_scores.csv', index_col = 'Unnamed: 0')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"factor_scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 1', ascending=False)[:11]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 1:\")\nfor index, row in top_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n# diary_count = top_10[top_10['type'] == 'diary']['type'].count()\n# postcard_count = top_10[top_10['type'] == 'postcard']['type'].count()\n\n# print(f\"Количество дневников в топ-100: {diary_count}\")\n# print(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = top_10[top_10['type'] == 'diary']['type'].count()\npostcard_count = top_10[top_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 1', ascending=True)[:20]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 1:\")\nfor index, row in bottom_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n# diary_count = bottom_10[bottom_10['type'] == 'diary']['type'].count()\n# postcard_count = bottom_10[bottom_10['type'] == 'postcard']['type'].count()\n\n# print(f\"Количество дневников в топ-100: {diary_count}\")\n# print(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = bottom_10[bottom_10['type'] == 'diary']['type'].count()\npostcard_count = bottom_10[bottom_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 2', ascending=False)[:10]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 2:\")\nfor index, row in top_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n# diary_count = top_10[top_10['type'] == 'diary']['type'].count()\n# postcard_count = top_10[top_10['type'] == 'postcard']['type'].count()\n\n# print(f\"Количество дневников в топ-100: {diary_count}\")\n# print(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = top_10[top_10['type'] == 'diary']['type'].count()\npostcard_count = top_10[top_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 2', ascending=True)[:20]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 2:\")\nfor index, row in bottom_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dif = feature_matrix_pr[feature_matrix_pr['text']==text]['avg_vp_length_abs'] / (feature_matrix_pr[feature_matrix_pr['text']==text]['text_length'] + 1)\n# (dif) * 100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 3', ascending=False)[:100]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 3:\")\nfor index, row in top_10.iterrows():\n    if row['type']=='postcard':\n        print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = top_10[top_10['type'] == 'diary']['type'].count()\npostcard_count = top_10[top_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 3', ascending=True)[:10]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 3:\")\nfor index, row in bottom_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = bottom_10[bottom_10['type'] == 'diary']['type'].count()\npostcard_count = bottom_10[bottom_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 4', ascending=False)[:100]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 4:\")\nfor index, row in top_10.iterrows():\n    if row['type'] == 'postcard':\n        print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 4', ascending=True)[:50]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 4:\")\nfor index, row in bottom_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 5', ascending=False)[:500]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 5:\")\nfor index, row in top_10.iterrows():\n    if row['type']=='diary':\n        print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = top_10[top_10['type'] == 'diary']['type'].count()\npostcard_count = top_10[top_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 5', ascending=True)[:200]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 5:\")\nfor index, row in bottom_10.iterrows():\n    if row['type']=='postcard':\n        print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 6', ascending=False)[:50]\n\n# Выводим тексты и значения, включая тип текста.\n\nprint(\"Тексты с наибольшими значениями Factor 6:\")\nfor index, row in top_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 6', ascending=True)[:50]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 6:\")\nfor index, row in bottom_10.iterrows():\n    if row['type']=='postcard':\n        print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 7', ascending=False)[:300]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 7:\")\nfor index, row in top_10.iterrows():\n    if row['type'] == 'diary': \n        print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = top_10[top_10['type'] == 'diary']['type'].count()\npostcard_count = top_10[top_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 7', ascending=True)[:100]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 7:\")\nfor index, row in bottom_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"diary_count = bottom_10[bottom_10['type'] == 'diary']['type'].count()\npostcard_count = bottom_10[bottom_10['type'] == 'postcard']['type'].count()\n\nprint(f\"Количество дневников в топ-100: {diary_count}\")\nprint(f\"Количество открыток в топ-100: {postcard_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 8', ascending=False)[:20]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 8:\")\nfor index, row in top_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 8', ascending=True)[:20]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 8:\")\nfor index, row in bottom_10.iterrows():\n    if row['type'] == 'postcard':\n        print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по убыванию Factor 1 и берем первые 10 строк\ntop_10 = factor_scores.sort_values(by='Factor 9', ascending=False)[:20]\n\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наибольшими значениями Factor 9:\")\nfor index, row in top_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сортируем по возрастанию Factor 1 и берем первые 10 строк\nbottom_10 = factor_scores.sort_values(by='Factor 9', ascending=True)[:50]\n\n# Выводим тексты и значения\n# Выводим тексты и значения, включая тип текста\nprint(\"Тексты с наименьшими значениями Factor 9:\")\nfor index, row in bottom_10.iterrows():\n    print(f\"Type: {row['type']}, Text: {row['text']}}}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Factor dinamics","metadata":{}},{"cell_type":"code","source":"factors = [f'Factor {i}' for i in range(1, 10)]  # Список факторов\n\n# Создаем фигуру с подграфиками\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\naxes = axes.flatten()  # Преобразуем массив подграфиков в одномерный для удобства\n\n# Перебираем факторы и создаем графики\nfor i, factor in enumerate(factors):\n    sns.lineplot(data=factor_scores, x='decade', y=factor, ax=axes[i])\n    axes[i].set_title(f'Динамика {factor} по декадам')\n    axes[i].set_xlabel('Декада')\n    axes[i].set_ylabel(factor)\n\nplt.tight_layout() \nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"factors = [f'Factor {i}' for i in range(1, 10)]\n\n# Создаем фигуру с подграфиками\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Перебираем факторы и создаем графики\nfor i, factor in enumerate(factors):\n    sns.lineplot(data=factor_scores, x='decade', y=factor, hue='type', ax=axes[i]) #hue параметр\n    axes[i].set_title(f'Динамика {factor} по декадам (Diary vs Postcard)')\n    axes[i].set_xlabel('Декада')\n    axes[i].set_ylabel(factor)\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfactors = [f'Factor {i}' for i in range(1, 10)]\n\n# Создаем фигуру с подграфиками\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Перебираем факторы и создаем графики\nfor i, factor in enumerate(factors):\n    sns.lineplot(data=factor_scores, x='decade', y=factor, hue='type', ax=axes[i], palette='gray')  # Указание палитры\n    axes[i].set_title(f'Динамика {factor} по декадам (Diary vs Postcard)')\n    axes[i].set_xlabel('Декада')\n    axes[i].set_ylabel(factor)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"factor_scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"factor_means_by_decade = factor_scores.copy().drop(columns=['text', 'type'], errors='ignore')\n\n# Группируем по декадам и вычисляем среднее\nfactor_means_by_decade = factor_means_by_decade.groupby('decade').mean(numeric_only=True)\n\n\n# Выводим таблицу\nfactor_means_by_decade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\n# Для каждого фактора строим линию\nfor factor in factor_means_by_decade.columns:\n    if factor.startswith('Factor'):\n        plt.plot(\n            factor_means_by_decade.index,\n            factor_means_by_decade[factor],\n            marker='o',\n            label=factor\n        )\n\nplt.title('Динамика средних факторных оценок по декадам')\nplt.xlabel('Десятилетие')\nplt.ylabel('Средняя факторная оценка')\nplt.legend()\nplt.grid(True, which='both', linestyle='--', linewidth=0.7, color='gray', alpha=0.7)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Группировка по типу текста и декаде\nfactor_means_by_type_decade = factor_scores.copy().drop(columns=['text'], errors='ignore')\n\n# Группировка и усреднение\nfactor_means_by_type_decade = factor_means_by_type_decade.groupby(['type', 'decade']).mean(numeric_only=True)\n\n# Просмотр таблицы\nfactor_means_by_type_decade\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Получаем список факторов\nfactor_columns = [col for col in factor_means_by_type_decade.columns if col.startswith('Factor')]\n\n# Получаем список типов текстов\ntext_types = factor_means_by_type_decade.index.get_level_values('type').unique()\n\nfor text_type in text_types:\n    # Отбираем данные для конкретного типа\n    data = factor_means_by_type_decade.loc[text_type]\n    \n    plt.figure(figsize=(12, 6))\n    \n    for factor in factor_columns:\n        plt.plot(\n            data.index,  # декады\n            data[factor],  # значения фактора\n            marker='o',\n            label=factor\n        )\n    \n    plt.title(f'Динамика средних факторных оценок по декадам для типа: {text_type}')\n    plt.xlabel('Декада')\n    plt.ylabel('Средняя факторная оценка')\n    plt.legend()\n    plt.grid(True, which='both', linestyle='--', linewidth=0.7, color='gray', alpha=0.7)\n    \n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.cluster import KMeans\n# from sklearn.preprocessing import StandardScaler\n\n# # 1. Выделяем только факторные оценки (без года)\n# data_for_clustering = cluster_by_factors[[col for col in cluster_by_factors.columns if col.startswith('Factor')]]\n\n# # 2. Стандартизация\n# scaler = StandardScaler()\n# data_scaled = scaler.fit_transform(data_for_clustering)\n\n# # 4. Кластеризация с оптимальным k (например, k=3)\n# kmeans = KMeans(n_clusters=2, random_state=42)\n# clusters = kmeans.fit_predict(data_scaled)\n# cluster_by_factors['Период'] = clusters\n\n# # 5. Визуализация по годам\n# plt.figure(figsize=(12, 6))\n# sns.boxplot(x='Период', y='year', data=cluster_by_factors)\n# plt.title('Распределение годов по кластерам')\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Temporal classification","metadata":{}},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"code","source":"counts = result_df.groupby(['decade', 'type']).size()\nprint(\"Исходное распределение:\\n\", counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bins_20 = list(range(1900, 2041, 20))  # от 1900 до 2020 с шагом 20 лет\nlabels_20 = [f'{start}-{start+19}' for start in bins_20[:-1]]  # метки для интервалов\nresult_df['decade_class_20yrs'] = pd.cut(result_df['decade'], bins=bins_20, labels=labels_20, right=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Указанные интервалы-исключения: последние два 20-летия\nspecial_classes = ['2000-2019', '2020-2039']\n\n# Создаем новый столбец для классификации по 20-летиям\nresult_df['decade_class_20yrs'] = pd.cut(result_df['decade'], bins=bins_20, labels=labels_20, right=False)\n\n# Разделяем датафрейм\ndf_rest = result_df[~result_df['decade_class_20yrs'].isin(special_classes)]\ndf_special = result_df[result_df['decade_class_20yrs'].isin(special_classes)]\n\n# Уровень выборки: 1600 для каждой 20-летней группы\nbalanced_samples = []\n\n# Получаем список уникальных классов из df_rest\nclasses_20yrs = df_rest['decade_class_20yrs'].unique()\n\nfor cls in classes_20yrs:\n    subset = df_rest[df_rest['decade_class_20yrs'] == cls]\n    n_samples = min(1600, len(subset))  # на случай, если текстов меньше\n    sampled = subset.sample(n=n_samples, random_state=42)\n    balanced_samples.append(sampled)\n\n# Добавляем все из special_classes без изменений\nbalanced_samples.append(df_special)\n\n# Объединяем\ndf_balanced = pd.concat(balanced_samples, ignore_index=True)\n\n# Выводим распределение\nprint(\"\\nРаспределение после выравнивания:\\n\", \n      df_balanced['decade_class_20yrs'].value_counts().sort_index())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Классы 20-летий, которые нужно оставить полностью\nspecial_classes = ['2000-2019', '2020-2039']\n\n# Создаем новый столбец с делением по 20-летиям\nresult_df['decade_class_20yrs'] = pd.cut(result_df['decade'], bins=bins_20, labels=labels_20, right=False)\n\n# Функция для создания сбалансированного датафрейма для одного типа\ndef create_balanced_df(df, text_type):\n    df_type = df[df['type'] == text_type]\n    \n    df_rest = df_type[~df_type['decade_class_20yrs'].isin(special_classes)]\n    df_special = df_type[df_type['decade_class_20yrs'].isin(special_classes)]\n    \n    balanced_samples = []\n    \n    classes_20yrs = df_rest['decade_class_20yrs'].unique()\n    \n    for cls in classes_20yrs:\n        subset = df_rest[df_rest['decade_class_20yrs'] == cls]\n        n_samples = min(1000, len(subset))  # чтобы не было ошибки при нехватке текстов\n        sampled = subset.sample(n=n_samples, random_state=42)\n        balanced_samples.append(sampled)\n    \n    # Добавляем все из special_classes без изменений\n    balanced_samples.append(df_special)\n    \n    # Объединяем\n    df_balanced = pd.concat(balanced_samples, ignore_index=True)\n    \n    return df_balanced\n\n# Создаем два отдельных датафрейма\npostcard_balanced = create_balanced_df(result_df, 'postcard')\ndiary_balanced = create_balanced_df(result_df, 'diary')\n\n# Проверка распределения\nprint(\"\\nРаспределение для postcard:\\n\", postcard_balanced['decade_class_20yrs'].value_counts().sort_index())\nprint(\"\\nРаспределение для diary:\\n\", diary_balanced['decade_class_20yrs'].value_counts().sort_index())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"descriptive_stats = df_balanced.describe().T[['mean', 'min', 'max', 'std']]\ndescriptive_stats['range'] = descriptive_stats['max'] - descriptive_stats['min']\n\n# Сохраняем в таблицу\ndescriptive_stats.to_csv('descriptive_stats.csv')\nprint(descriptive_stats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in nan_cols:\n    df_balanced[col] = df_balanced[col].fillna(0).astype('float64')\n\nnan_cols = df_balanced.columns[df_balanced.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\ndf_with_nan = df_balanced[nan_cols]\n\n# Создаем булеву маску, где True соответствует строкам, содержащим хотя бы один NaN\nrows_with_nan = df_with_nan.isnull().any(axis=1)\n\n# Считаем количество строк, содержащих хотя бы один NaN\nprint(rows_with_nan.sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix  = df_balanced.copy()\nstandardized_feature_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix  = standardized_feature_matrix.drop(['text', 'year', 'decade', 'type', 'text_length', 'decade_class_20yrs'], axis=1)\nstandardized_feature_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in standardized_feature_matrix.columns:\n    mean = descriptive_stats.loc[column, 'mean']\n    std = descriptive_stats.loc[column, 'std']\n\n    standardized_feature_matrix[column] = (standardized_feature_matrix[column] - mean) / std\nstandardized_feature_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_columns_new = [col for col in standardized_feature_matrix.columns]\nlen(feature_columns_new)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Среднее после стандартизации должно быть ~0\nprint(standardized_feature_matrix[feature_columns_new].mean().round(2))  \n\n# Стандартное отклонение должно быть ~1\nprint(standardized_feature_matrix[feature_columns_new].std().round(2))   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Создаем DataFrame factor_scores с текстами и декадами\nfactor_scores = df_balanced[['text', 'decade', 'type', 'decade_class_20yrs']].copy()\n\n# 2. Связываем standardized_feature_matrix с factor_scores по индексам\nstandardized_features = standardized_feature_matrix.copy()\nstandardized_features.index = factor_scores.index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Функция для расчета факторной оценки\ndef calculate_factor_score(row, factor_features):\n    score = 0\n    for feature, loading in factor_features:\n        # Получаем стандартизированное значение признака\n        value = row[feature]\n        # Учитываем знак нагрузки: + если loading > 0, - если loading < 0\n        score += value * (1 if loading > 0 else -1)\n    return score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Для каждого фактора из factor_dict вычисляем оценку\nfor factor, features in factor_dict.items():\n    # Признаки и их нагрузки для текущего фактора\n    factor_features = [(feat[0], feat[1]) for feat in features]\n\n    # Проверяем наличие признаков в standardized_features\n    missing = [feat[0] for feat in features if feat[0] not in standardized_features.columns]\n    if missing:\n        print(f\"Предупреждение: Признаки {missing} отсутствуют в standardized_feature_matrix.\")\n        continue\n\n    # Рассчитываем факторную оценку\n    factor_scores[factor] = standardized_features.apply(\n        lambda row: calculate_factor_score(row, factor_features), axis=1\n    )\n\nfactor_scores.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# factor_scores = pd.read_csv('/kaggle/input/normal-data/factor_scores.csv', index_col='Unnamed: 0').drop(columns=['decade_class'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification","metadata":{}},{"cell_type":"code","source":"# Перемешиваем данные\nfactor_scores_shuffled = factor_scores.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Проверяем результат\nfactor_scores_shuffled","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nРаспределение после выравнивания:\\n\", \n      factor_scores_shuffled.groupby(['decade_class_20yrs']).size())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# encoder = LabelEncoder()\n# factor_scores_shuffled[\"decade_binary_encoded\"] = encoder.fit_transform(factor_scores_shuffled[\"decade_class_binary\"])\n# target_decade = factor_scores_shuffled[\"decade_binary_encoded\"].values\n\nencoder_20yrs = LabelEncoder()\nfactor_scores_shuffled[\"decade_20yrs_encoded\"] = encoder_20yrs.fit_transform(factor_scores_shuffled[\"decade_class_20yrs\"])\ntarget_20yrs = factor_scores_shuffled[\"decade_20yrs_encoded\"].values\n\n\n# encoder_10yrs = LabelEncoder()\n# factor_scores_shuffled[\"decade_encoded\"] = encoder_10yrs.fit_transform(factor_scores_shuffled[\"decade\"])\n# target_10yrs = factor_scores_shuffled[\"decade_encoded\"].values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Выделяем только факторные признаки\nfactor_features_df = factor_scores_shuffled.filter(regex=\"Factor\").copy()\n\n# Сохраняем имена колонок\nfactor_columns = factor_features_df.columns.tolist()\n\n# Получаем значения признаков\nfactor_features = factor_features_df.values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 20s classification","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_full_20s, X_test_20s, y_train_full_20s, y_test_20s = train_test_split(\n    factor_features, target_20yrs, \n    test_size=0.2, \n    stratify=target_20yrs, \n    random_state=42\n)\n\n\nX_train_20s, X_val_20s, y_train_20s, y_val_20s = train_test_split(\n    X_train_full_20s, y_train_full_20s, \n    test_size=0.2, \n    stratify=y_train_full_20s, \n    random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model comparison","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    ExtraTreesClassifier,\n    GradientBoostingClassifier,\n    HistGradientBoostingClassifier\n)\nfrom sklearn.metrics import classification_report, f1_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport numpy as np\n\n# Инициализация моделей\ntree_models = {\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'Extra Trees': ExtraTreesClassifier(random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n    'Hist Gradient Boosting': HistGradientBoostingClassifier(random_state=42),\n    'LightGBM': lgb.LGBMClassifier(random_state=42),\n    'XGBoost': xgb.XGBClassifier(random_state=42)\n}\n\nf1_scores_dict = {}\n\nfor name, model in tree_models.items():\n    print(f\"\\n{'='*30} {name} {'='*30}\")\n    \n    # Кросс-валидация на тренировочной выборке\n    f1_scores = cross_val_score(model, X_train_20s, y_train_20s,\n                                scoring='f1_macro', cv=5, n_jobs=-1)\n    mean_f1 = np.mean(f1_scores)\n    f1_scores_dict[name] = mean_f1\n    print(f\"Cross-validated macro-F1: {mean_f1:.3f}\")\n\n# Определим лучшую модель\nbest_model_name = max(f1_scores_dict, key=f1_scores_dict.get)\nprint(f\"\\n Лучшая модель по cross-val F1_macro: {best_model_name} ({f1_scores_dict[best_model_name]:.3f})\")\n\n# Инициализация лучшей модели\nbest_model = tree_models[best_model_name]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Grid search","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, f1_score\n\n# Инициализация модели\nrf = RandomForestClassifier(random_state=42)\n\n# Сетка гиперпараметров\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 20, None],\n    'max_features': ['sqrt', 0.8],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2],\n    'bootstrap': [True, False]\n}\n\n# GridSearchCV с метрикой macro F1\ngrid_search = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    cv=5,\n    scoring='f1_macro',\n    n_jobs=-1,\n    verbose=2\n)\n\n# Обучение\ngrid_search.fit(X_train_20s, y_train_20s)\n\n# Лучшая модель и её гиперпараметры\nprint(\"\\nЛучшие параметры:\", grid_search.best_params_)\nbest_model = grid_search.best_estimator_\n\n# Оценка на валидационной выборке\nval_pred = best_model.predict(X_val_20s)\nprint(\"\\nValidation Classification Report:\")\nprint(classification_report(y_val_20s, val_pred, digits=3))\n\n# Оценка на тестовой выборке\ntest_pred = best_model.predict(X_test_20s)\nprint(\"\\nTest Classification Report:\")\nprint(classification_report(y_test_20s, test_pred, digits=3))\n\nprint(f\"\\nMacro-F1 (Validation): {f1_score(y_val_20s, val_pred, average='macro'):.3f}\")\nprint(f\"Macro-F1 (Test): {f1_score(y_test_20s, test_pred, average='macro'):.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nbest_model = RandomForestClassifier(min_samples_leaf=2, n_estimators=200, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training best model","metadata":{}},{"cell_type":"code","source":"best_model.fit(X_train_20s, y_train_20s)\n\n\ny_val_pred = best_model.predict(X_val_20s)\nprint(\"Classification report for postcards (Validation):\")\nprint(classification_report(y_val_20s, y_val_pred, digits=3))\nmacro_f1_val = f1_score(y_val_20s, y_val_pred, average='macro')\nprint(f\"Macro F1 for postcards (Validation): {macro_f1_val:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.precision', 3)\nimportances = best_model.feature_importances_\n\nfeature_importance_df = pd.DataFrame({'Feature': factor_columns, 'Importance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\nfeature_importance_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install shap\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\n\n# Инициализация Explainer\nexplainer = shap.TreeExplainer(best_model)\n\n# Вычисление значений SHAP\nshap_values = explainer.shap_values(X_val_20s)  # X_val — валидационные признаки (без таргета)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Создаем сетку графиков\nn_classes = len(shap_values)\nn_cols = 3  # Колонок в ряду\nn_rows = (n_classes + n_cols - 1) // n_cols  # Вычисляем нужное количество рядов\n\nplt.figure(figsize=(n_cols*6, n_rows*4))  # Размер фигуры\n\nfor i in range(n_classes):\n    plt.subplot(n_rows, n_cols, i+1)  # Позиция графика в сетке\n    shap.summary_plot(\n        shap_values[i], \n        X_val_20s, \n        cmap=plt.cm.Greys, # Градиент от белого к черному\n        feature_names=factor_columns,\n        show=False,  # Не показывать сразу\n        plot_size=None  # Отключить авторазмер\n    )\n    plt.title(f\"Class {i}\", fontsize=12)\n    plt.gcf().tight_layout()  # Оптимизация расположения\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Создаем сетку графиков\nn_classes = len(shap_values)\nn_cols = 3  # Колонок в ряду\nn_rows = (n_classes + n_cols - 1) // n_cols  # Вычисляем нужное количество рядов\n\nplt.figure(figsize=(n_cols*6, n_rows*4))  # Размер фигуры\n\nfor i in range(n_classes):\n    plt.subplot(n_rows, n_cols, i+1)  # Позиция графика в сетке\n    shap.summary_plot(\n        shap_values[i], \n        X_val_20s,  \n        feature_names=factor_columns,\n        show=False,  # Не показывать сразу\n        plot_size=None  # Отключить авторазмер\n    )\n    plt.title(f\"Class {i}\", fontsize=12)\n    plt.gcf().tight_layout()  # Оптимизация расположения\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score\n\n# Распределение классов в обучающих данных\nclass_counts = np.bincount(y_train_20s)\nclass_probabilities = class_counts / len(y_train_20s)\nclasses = np.arange(len(class_probabilities))\n\n# Количество прогонов для оценки матожидания\nn_runs = 1000\nmacro_f1_scores = []\n\nfor _ in range(n_runs):\n    # Случайные предсказания согласно распределению классов\n    y_pred_random = np.random.choice(classes, size=len(y_test_20s), p=class_probabilities)\n    \n    # Расчет macro-F1\n    f1 = f1_score(y_test_20s, y_pred_random, average=\"macro\")\n    macro_f1_scores.append(f1)\n\n# Ожидаемый macro-F1\nexpected_macro_f1 = np.mean(macro_f1_scores)\nprint(f\"Ожидаемый Macro-F1: {expected_macro_f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"most_frequent_class = np.argmax(class_probabilities)\ny_pred_constant = np.full(len(y_test_20s), most_frequent_class)\n\nconstant_macro_f1 = f1_score(y_test_20s, y_pred_constant, average=\"macro\")\nprint(f\"Macro-F1 константного классификатора: {constant_macro_f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Построение матрицы ошибок\ncm = confusion_matrix(y_val_20s, val_pred)\n\n# Нормализация по строкам и перевод в проценты\ncm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n\n# Подписи классов\ndecade_labels = [str(decade) for decade in encoder_20yrs.classes_]\n\n# Визуализация\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Greys',\n            xticklabels=decade_labels, yticklabels=decade_labels,\n            cbar_kws={'label': 'Percentage (%)'})\n\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.title(\"Confusion Matrix (Percent, Validation Set)\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models for register types ","metadata":{"_kg_hide-output":false}},{"cell_type":"code","source":"print(\"\\nРаспределение для postcard:\\n\", postcard_balanced['decade_class_20yrs'].value_counts().sort_index())\nprint(\"\\nРаспределение для diary:\\n\", diary_balanced['decade_class_20yrs'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### postcards","metadata":{}},{"cell_type":"code","source":"descriptive_stats_pt = postcard_balanced.describe().T[['mean', 'min', 'max', 'std']]\ndescriptive_stats_pt['range'] = descriptive_stats_pt['max'] - descriptive_stats_pt['min']\n\n# Сохраняем в таблицу\ndescriptive_stats_pt.to_csv('descriptive_stats.csv')\nprint(descriptive_stats_pt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in nan_cols:\n    postcard_balanced[col] = postcard_balanced[col].fillna(0).astype('float64')\n\nnan_cols = postcard_balanced.columns[postcard_balanced.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\ndf_with_nan = postcard_balanced[nan_cols]\n\n# Создаем булеву маску, где True соответствует строкам, содержащим хотя бы один NaN\nrows_with_nan = df_with_nan.isnull().any(axis=1)\n\n# Считаем количество строк, содержащих хотя бы один NaN\nprint(rows_with_nan.sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix_pt  = postcard_balanced.copy()\nstandardized_feature_matrix_pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix_pt  = standardized_feature_matrix_pt.drop(['text', 'year', 'decade', 'type', 'text_length', 'decade_class_20yrs'], axis=1)\nstandardized_feature_matrix_pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in standardized_feature_matrix_pt.columns:\n    mean = descriptive_stats_pt.loc[column, 'mean']\n    std = descriptive_stats_pt.loc[column, 'std']\n\n    standardized_feature_matrix_pt[column] = (standardized_feature_matrix_pt[column] - mean) / std\nstandardized_feature_matrix_pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Среднее после стандартизации должно быть ~0\nprint(standardized_feature_matrix_pt[feature_columns_new].mean().round(2))  \n\n# Стандартное отклонение должно быть ~1\nprint(standardized_feature_matrix_pt[feature_columns_new].std().round(2))   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Создаем DataFrame factor_scores с текстами и декадами\nfactor_scores_pt = postcard_balanced[['text', 'decade', 'type', 'decade_class_20yrs']].copy()\n\n# 2. Связываем standardized_feature_matrix с factor_scores по индексам\nstandardized_features_pt = standardized_feature_matrix_pt.copy()\nstandardized_features_pt.index = factor_scores_pt.index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Для каждого фактора из factor_dict вычисляем оценку\nfor factor, features in factor_dict.items():\n    # Признаки и их нагрузки для текущего фактора\n    factor_features = [(feat[0], feat[1]) for feat in features]\n\n    # Проверяем наличие признаков в standardized_features\n    missing = [feat[0] for feat in features if feat[0] not in standardized_features_pt.columns]\n    if missing:\n        print(f\"Предупреждение: Признаки {missing} отсутствуют в standardized_feature_matrix.\")\n        continue\n\n    # Рассчитываем факторную оценку\n    factor_scores_pt[factor] = standardized_features_pt.apply(\n        lambda row: calculate_factor_score(row, factor_features), axis=1\n    )\n\nfactor_scores_pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Перемешиваем данные\nfactor_scores_shuffled_pt = factor_scores_pt.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Проверяем результат\nfactor_scores_shuffled_pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_20yrs = LabelEncoder()\nfactor_scores_shuffled_pt[\"decade_20yrs_encoded\"] = encoder_20yrs.fit_transform(factor_scores_shuffled_pt[\"decade_class_20yrs\"])\ntarget_20yrs_pt = factor_scores_shuffled_pt[\"decade_20yrs_encoded\"].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Выделяем только факторные признаки\nfactor_features_df_pt = factor_scores_shuffled_pt.filter(regex=\"Factor\").copy()\n\n# Сохраняем имена колонок\nfactor_columns = factor_features_df_pt.columns.tolist()\n\n# Получаем значения признаков\nfactor_features_pt = factor_features_df_pt.values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_full_20s_pt, X_test_20s_pt, y_train_full_20s_pt, y_test_20s_pt = train_test_split(\n    factor_features_pt, target_20yrs_pt, \n    test_size=0.2, \n    stratify=target_20yrs_pt, \n    random_state=42\n)\n\n\nX_train_20s_pt, X_val_20s_pt, y_train_20s_pt, y_val_20s_pt = train_test_split(\n    X_train_full_20s_pt, y_train_full_20s_pt, \n    test_size=0.2, \n    stratify=y_train_full_20s_pt, \n    random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Инициализация моделей с одинаковыми параметрами\nrf_postcards = RandomForestClassifier(min_samples_leaf=2, n_estimators=200, random_state=42)\n\n# Обучение модели для открыток\nrf_postcards.fit(X_train_20s_pt, y_train_20s_pt)\n\n\ny_val_pred_postcards = rf_postcards.predict(X_val_20s_pt)\nprint(\"Classification report for postcards (Validation):\")\nprint(classification_report(y_val_20s_pt, y_val_pred_postcards, digits=3))\nmacro_f1_postcards_val = f1_score(y_val_20s_pt, y_val_pred_postcards, average='macro')\nprint(f\"Macro F1 for postcards (Validation): {macro_f1_postcards_val:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.precision', 3)\nimportances = rf_postcards.feature_importances_\n\nfeature_importance_df = pd.DataFrame({'Feature': factor_columns, 'Importance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\nfeature_importance_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### diaries ","metadata":{}},{"cell_type":"code","source":"descriptive_stats_pr = diary_balanced.describe().T[['mean', 'min', 'max', 'std']]\ndescriptive_stats_pr['range'] = descriptive_stats_pr['max'] - descriptive_stats_pr['min']\n\n# Сохраняем в таблицу\ndescriptive_stats_pr.to_csv('descriptive_stats.csv')\nprint(descriptive_stats_pr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in nan_cols:\n    diary_balanced[col] = diary_balanced[col].fillna(0).astype('float64')\n\nnan_cols = diary_balanced.columns[diary_balanced.isnull().any()]\nprint(\"Столбцы, содержащие NaN:\", nan_cols)\n\ndf_with_nan = diary_balanced[nan_cols]\n\n# Создаем булеву маску, где True соответствует строкам, содержащим хотя бы один NaN\nrows_with_nan = df_with_nan.isnull().any(axis=1)\n\n# Считаем количество строк, содержащих хотя бы один NaN\nprint(rows_with_nan.sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix_pr  = diary_balanced.copy()\nstandardized_feature_matrix_pr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"standardized_feature_matrix_pr  = standardized_feature_matrix_pr.drop(['text', 'year', 'decade', 'type', 'text_length', 'decade_class_20yrs'], axis=1)\nstandardized_feature_matrix_pr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in standardized_feature_matrix_pr.columns:\n    mean = descriptive_stats_pr.loc[column, 'mean']\n    std = descriptive_stats_pr.loc[column, 'std']\n\n    standardized_feature_matrix_pr[column] = (standardized_feature_matrix_pr[column] - mean) / std\nstandardized_feature_matrix_pr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Среднее после стандартизации должно быть ~0\nprint(standardized_feature_matrix_pr[feature_columns_new].mean().round(2))  \n\n# Стандартное отклонение должно быть ~1\nprint(standardized_feature_matrix_pr[feature_columns_new].std().round(2))   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Создаем DataFrame factor_scores с текстами и декадами\nfactor_scores_pr = diary_balanced[['text', 'decade', 'type', 'decade_class_20yrs']].copy()\n\n# 2. Связываем standardized_feature_matrix с factor_scores по индексам\nstandardized_features_pr = standardized_feature_matrix_pr.copy()\nstandardized_features_pr.index = factor_scores_pr.index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Для каждого фактора из factor_dict вычисляем оценку\nfor factor, features in factor_dict.items():\n    # Признаки и их нагрузки для текущего фактора\n    factor_features = [(feat[0], feat[1]) for feat in features]\n\n    # Проверяем наличие признаков в standardized_features\n    missing = [feat[0] for feat in features if feat[0] not in standardized_features_pr.columns]\n    if missing:\n        print(f\"Предупреждение: Признаки {missing} отсутствуют в standardized_feature_matrix.\")\n        continue\n\n    # Рассчитываем факторную оценку\n    factor_scores_pr[factor] = standardized_features_pr.apply(\n        lambda row: calculate_factor_score(row, factor_features), axis=1\n    )\n\nfactor_scores_pr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Перемешиваем данные\nfactor_scores_shuffled_pr = factor_scores_pr.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Проверяем результат\nfactor_scores_shuffled_pr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_20yrs = LabelEncoder()\nfactor_scores_shuffled_pr[\"decade_20yrs_encoded\"] = encoder_20yrs.fit_transform(factor_scores_shuffled_pr[\"decade_class_20yrs\"])\ntarget_20yrs_pr = factor_scores_shuffled_pr[\"decade_20yrs_encoded\"].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Выделяем только факторные признаки\nfactor_features_df_pr = factor_scores_shuffled_pr.filter(regex=\"Factor\").copy()\n\n# Сохраняем имена колонок\nfactor_columns = factor_features_df_pr.columns.tolist()\n\n# Получаем значения признаков\nfactor_features_pr = factor_features_df_pr.values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_full_20s_pr, X_test_20s_pr, y_train_full_20s_pr, y_test_20s_pr = train_test_split(\n    factor_features_pr, target_20yrs_pr, \n    test_size=0.2, \n    stratify=target_20yrs_pr, \n    random_state=42\n)\n\n\nX_train_20s_pr, X_val_20s_pr, y_train_20s_pr, y_val_20s_pr = train_test_split(\n    X_train_full_20s_pr, y_train_full_20s_pr, \n    test_size=0.2, \n    stratify=y_train_full_20s_pr, \n    random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Инициализация моделей с одинаковыми параметрами\nrf_diaries = RandomForestClassifier(min_samples_leaf=2, n_estimators=200, random_state=42)\n\n# Обучение модели для открыток\nrf_diaries.fit(X_train_20s_pr, y_train_20s_pr)\n\n\ny_val_pred_diaries = rf_diaries.predict(X_val_20s_pr)\nprint(\"Classification report for postcards (Validation):\")\nprint(classification_report(y_val_20s_pr, y_val_pred_diaries, digits=3))\nmacro_f1_diaires_val = f1_score(y_val_20s_pr, y_val_pred_diaries, average='macro')\nprint(f\"Macro F1 for postcards (Validation): {macro_f1_diaires_val:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.precision', 3)\nimportances = rf_diaries.feature_importances_\n\nfeature_importance_df = pd.DataFrame({'Feature': factor_columns, 'Importance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\nfeature_importance_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"----------------------","metadata":{}}]}