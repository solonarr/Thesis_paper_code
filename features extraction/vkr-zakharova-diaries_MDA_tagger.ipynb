{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1JC4YQnZxt-yL80YUQLdn_ATK5cSorQX5","timestamp":1746037618758}],"gpuType":"T4","mount_file_id":"14MRBii4dJZd8ndkLpHRFyKtuOPhTH9NT","authorship_tag":"ABX9TyOiE0a1xjQA8Y/yEmjB/sg8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9cab5090751844b785a421bf73050107":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbfb092a833347b8af8f4ac6411e0c42","IPY_MODEL_f8dca0210b914da3878bfbb2b6f4376b","IPY_MODEL_8f80b9008bc344449b78cdb213804dec"],"layout":"IPY_MODEL_a70180a5143241ca9a565efd44d2fd54"}},"cbfb092a833347b8af8f4ac6411e0c42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d2497e8c5d04e6bb47eee5100bd8148","placeholder":"​","style":"IPY_MODEL_2574897c76e84f74bf1723f03f4a763c","value":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "}},"f8dca0210b914da3878bfbb2b6f4376b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0351e180baf544f1900f2d4353fbf3db","max":52741,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f51f179a6e24c22b619fffcea320408","value":52741}},"8f80b9008bc344449b78cdb213804dec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5db94c552dc141d6a4153f699c14e952","placeholder":"​","style":"IPY_MODEL_72e126f1217f436398e12a0622586286","value":" 426k/? [00:00&lt;00:00, 32.5MB/s]"}},"a70180a5143241ca9a565efd44d2fd54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d2497e8c5d04e6bb47eee5100bd8148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2574897c76e84f74bf1723f03f4a763c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0351e180baf544f1900f2d4353fbf3db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f51f179a6e24c22b619fffcea320408":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5db94c552dc141d6a4153f699c14e952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72e126f1217f436398e12a0622586286":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7070064faf54468aa07ae7202353319a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97b6a48935a54d8a8170eb4d7435341f","IPY_MODEL_852422a7b83f46788dcc059b7ca5215b","IPY_MODEL_9d9c03faf9334a0fb665e288f36acab2"],"layout":"IPY_MODEL_defe1b103dc34eb283f0fc640efae97c"}},"97b6a48935a54d8a8170eb4d7435341f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8c1e195ab06422f8228b681cc4d0f01","placeholder":"​","style":"IPY_MODEL_7020d2919e584ff0bab3cd67fe132a33","value":"Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/default.zip: 100%"}},"852422a7b83f46788dcc059b7ca5215b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e6b64f659a94189850c35d27e90da4b","max":629165195,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e43d6f6f481457f818da29109ed6931","value":629165195}},"9d9c03faf9334a0fb665e288f36acab2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12ec332189414b2fab08d2997abf86d3","placeholder":"​","style":"IPY_MODEL_6735697e4bba4aaeb9fbb9e530850d98","value":" 629M/629M [00:02&lt;00:00, 321MB/s]"}},"defe1b103dc34eb283f0fc640efae97c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8c1e195ab06422f8228b681cc4d0f01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7020d2919e584ff0bab3cd67fe132a33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e6b64f659a94189850c35d27e90da4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e43d6f6f481457f818da29109ed6931":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12ec332189414b2fab08d2997abf86d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6735697e4bba4aaeb9fbb9e530850d98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e27acd81a413448f8dc9c151be7fd24d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_881e11c007974154b6a54a96d265a7db","IPY_MODEL_b19dea45121b4bc5a93190e262a90893","IPY_MODEL_44a190e83a4f4b8fb60658cf30eb1b84"],"layout":"IPY_MODEL_cf181b4c993b4897bad608ec53224c45"}},"881e11c007974154b6a54a96d265a7db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f65a5121260d467597569cc3b56f08ee","placeholder":"​","style":"IPY_MODEL_b922faf7d12c4fa8b8d1d6aee1b94769","value":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "}},"b19dea45121b4bc5a93190e262a90893":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c064f128bd954cc8a4dba2b9f496a7f6","max":52741,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b0a1501eb8b47f39e11e12fdf2944bf","value":52741}},"44a190e83a4f4b8fb60658cf30eb1b84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c110f802f78c42b2956f3f04348505b4","placeholder":"​","style":"IPY_MODEL_3d6b3d2da5cf497d8b0d370291425931","value":" 426k/? [00:00&lt;00:00, 37.3MB/s]"}},"cf181b4c993b4897bad608ec53224c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f65a5121260d467597569cc3b56f08ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b922faf7d12c4fa8b8d1d6aee1b94769":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c064f128bd954cc8a4dba2b9f496a7f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b0a1501eb8b47f39e11e12fdf2944bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c110f802f78c42b2956f3f04348505b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d6b3d2da5cf497d8b0d370291425931":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11634789,"sourceType":"datasetVersion","datasetId":7299924},{"sourceId":11715020,"sourceType":"datasetVersion","datasetId":7353537},{"sourceId":11783893,"sourceType":"datasetVersion","datasetId":7398494},{"sourceId":11869769,"sourceType":"datasetVersion","datasetId":7459227}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"E-HwqAyQC2zg"}},{"cell_type":"code","source":"!pip install pymorphy3","metadata":{"id":"nnMjZHZxHRbA","executionInfo":{"status":"ok","timestamp":1746037790878,"user_tz":-180,"elapsed":3143,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"87617e56-9985-4835-8ee1-36504e3dd04f","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:12:11.861368Z","iopub.execute_input":"2025-05-19T12:12:11.861705Z","iopub.status.idle":"2025-05-19T12:12:18.422099Z","shell.execute_reply.started":"2025-05-19T12:12:11.861675Z","shell.execute_reply":"2025-05-19T12:12:18.420281Z"}},"outputs":[{"name":"stdout","text":"Collecting pymorphy3\n  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\nCollecting dawg2-python>=0.8.0 (from pymorphy3)\n  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\nCollecting pymorphy3-dicts-ru (from pymorphy3)\n  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\nDownloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\nDownloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\nSuccessfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom pymorphy3 import MorphAnalyzer\nimport pymorphy3\nimport json\nimport numpy as np\n","metadata":{"id":"o_2s5EThC2C_","executionInfo":{"status":"ok","timestamp":1746037832774,"user_tz":-180,"elapsed":3,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:12:18.425033Z","iopub.execute_input":"2025-05-19T12:12:18.425421Z","iopub.status.idle":"2025-05-19T12:12:18.833748Z","shell.execute_reply.started":"2025-05-19T12:12:18.425383Z","shell.execute_reply":"2025-05-19T12:12:18.832425Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nwith open(\"/kaggle/input/vkr-data/diary_entries_final (1).json\", \"r\", encoding=\"utf-8\") as file:\n    data = json.load(file)","metadata":{"id":"K-NRee26CIbb","executionInfo":{"status":"ok","timestamp":1746037834701,"user_tz":-180,"elapsed":254,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:31.818487Z","iopub.execute_input":"2025-05-12T09:16:31.819125Z","iopub.status.idle":"2025-05-12T09:16:32.324672Z","shell.execute_reply.started":"2025-05-12T09:16:31.819100Z","shell.execute_reply":"2025-05-12T09:16:32.323844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_old_keys(json_data):\n    \"\"\"Удаляет из JSON все ключи, которые меньше 1900\"\"\"\n    return {key: value for key, value in json_data.items() if int(key) >= 1900}\n\nfiltered_data = remove_old_keys(data)\n","metadata":{"id":"-hXd9_-G9xEy","executionInfo":{"status":"ok","timestamp":1746037839802,"user_tz":-180,"elapsed":3,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:35.800989Z","iopub.execute_input":"2025-05-12T09:16:35.801738Z","iopub.status.idle":"2025-05-12T09:16:35.805920Z","shell.execute_reply.started":"2025-05-12T09:16:35.801711Z","shell.execute_reply":"2025-05-12T09:16:35.805253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows = []\nfor year, texts in filtered_data.items():\n    for text in texts:\n        rows.append({\"year\": int(year), \"text\": text})\n\ndf = pd.DataFrame(rows)\n\n","metadata":{"id":"MoCzwxrl9zOu","executionInfo":{"status":"ok","timestamp":1746037849091,"user_tz":-180,"elapsed":82,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:37.338731Z","iopub.execute_input":"2025-05-12T09:16:37.339302Z","iopub.status.idle":"2025-05-12T09:16:37.370096Z","shell.execute_reply.started":"2025-05-12T09:16:37.339262Z","shell.execute_reply":"2025-05-12T09:16:37.369347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"id":"sNwdbKtb91Xe","executionInfo":{"status":"ok","timestamp":1746037857119,"user_tz":-180,"elapsed":676,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"409d1d7e-8031-4b86-f87f-3cf45d95ce94","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:39.179697Z","iopub.execute_input":"2025-05-12T09:16:39.179959Z","iopub.status.idle":"2025-05-12T09:16:39.213273Z","shell.execute_reply.started":"2025-05-12T09:16:39.179938Z","shell.execute_reply":"2025-05-12T09:16:39.212485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"id":"X7pDxjv8DbNn","executionInfo":{"status":"ok","timestamp":1746037851753,"user_tz":-180,"elapsed":47,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"5a5b386d-da8a-44dc-b19b-41660bf1bd1c","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:41.684722Z","iopub.execute_input":"2025-05-12T09:16:41.684989Z","iopub.status.idle":"2025-05-12T09:16:41.705248Z","shell.execute_reply.started":"2025-05-12T09:16:41.684970Z","shell.execute_reply":"2025-05-12T09:16:41.704406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"decade\"] = (df[\"year\"] // 10) * 10\ndecade_counts = df[\"decade\"].value_counts().sort_index()\ndecade_counts\n","metadata":{"id":"NuVaLz5Q942E","executionInfo":{"status":"ok","timestamp":1746037882237,"user_tz":-180,"elapsed":72,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"1fa81d19-0c77-4981-aad6-1593311ef77e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:43.234173Z","iopub.execute_input":"2025-05-12T09:16:43.234838Z","iopub.status.idle":"2025-05-12T09:16:43.246164Z","shell.execute_reply.started":"2025-05-12T09:16:43.234806Z","shell.execute_reply":"2025-05-12T09:16:43.245517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"id":"zy5WMDCu99rf","executionInfo":{"status":"ok","timestamp":1746037891219,"user_tz":-180,"elapsed":467,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"5b9fe783-be14-481a-bf55-c8721aa33c76","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T07:43:18.107045Z","iopub.execute_input":"2025-05-06T07:43:18.107784Z","iopub.status.idle":"2025-05-06T07:43:18.119668Z","shell.execute_reply.started":"2025-05-06T07:43:18.107721Z","shell.execute_reply":"2025-05-06T07:43:18.118877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Register analysis","metadata":{"id":"79z-4q5ZR6sM"}},{"cell_type":"code","source":"df_filtered_copy = df.copy()","metadata":{"id":"WbY5PGAoG9P8","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:45.926320Z","iopub.execute_input":"2025-05-12T09:16:45.927057Z","iopub.status.idle":"2025-05-12T09:16:45.932412Z","shell.execute_reply.started":"2025-05-12T09:16:45.927027Z","shell.execute_reply":"2025-05-12T09:16:45.931699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nimport nltk\n\nnltk.download('stopwords')\nmorph = MorphAnalyzer()\nrussian_stopwords = stopwords.words('russian') + ['это', 'весь', 'который']\n\ndef preprocess(text):\n    # Удаление специальных символов и чисел\n    text = re.sub(r'[^а-яёА-ЯЁ]', ' ', text.lower())\n    \n    # Токенизация и лемматизация\n    tokens = [morph.parse(word)[0].normal_form \n             for word in text.split() \n             if len(word) > 2 \n             and word not in russian_stopwords]\n    \n    return tokens\n\ndf_filtered_copy['processed'] = df_filtered_copy['text'].apply(preprocess)\ndf_filtered_copy","metadata":{"id":"NTRwE35oG4Y5","executionInfo":{"status":"ok","timestamp":1745958177957,"user_tz":-180,"elapsed":47,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"f4ed251c-5d2f-4085-bdff-7ff0e4e1167e","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:16:47.914440Z","iopub.execute_input":"2025-05-12T09:16:47.914706Z","iopub.status.idle":"2025-05-12T09:19:22.016384Z","shell.execute_reply.started":"2025-05-12T09:16:47.914687Z","shell.execute_reply":"2025-05-12T09:19:22.015590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim import corpora, models\nimport pyLDAvis.gensim_models\n\n# Создание словаря и корпуса\ndictionary = corpora.Dictionary(df_filtered_copy['processed'])\ncorpus = [dictionary.doc2bow(text) for text in df_filtered_copy['processed']]\n\n# Обучение LDA модели\nlda_model = models.LdaModel(\n    corpus=corpus,\n    id2word=dictionary,\n    num_topics=15,  # Экспериментируйте с количеством тем\n    passes=10,\n    alpha='auto'\n)\n\n# Визуализация\nvis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\npyLDAvis.display(vis)","metadata":{"id":"QHBJLO9lHOpO","executionInfo":{"status":"ok","timestamp":1745958184950,"user_tz":-180,"elapsed":7000,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"1beacae0-992a-4897-ca6a-0395e44a49d0","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:21:19.978750Z","iopub.execute_input":"2025-05-12T09:21:19.979420Z","iopub.status.idle":"2025-05-12T09:23:07.067515Z","shell.execute_reply.started":"2025-05-12T09:21:19.979390Z","shell.execute_reply":"2025-05-12T09:23:07.066850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import CoherenceModel\n\n# Расчет когерентности\ncoherence_model = CoherenceModel(\n    model=lda_model,\n    texts=df_filtered_copy['processed'],\n    dictionary=dictionary,\n    coherence='c_v'\n)\n\ncoherence_score = coherence_model.get_coherence()\nprint(f\"Coherence Score (C_v): {coherence_score:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:32:13.111882Z","iopub.execute_input":"2025-05-04T16:32:13.112150Z","iopub.status.idle":"2025-05-04T16:32:31.244387Z","shell.execute_reply.started":"2025-05-04T16:32:13.112130Z","shell.execute_reply":"2025-05-04T16:32:31.243688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# Создание временных срезов\nyearly_topics = defaultdict(list)\nfor year, text in zip(df_filtered_copy['year'], df_filtered_copy['processed']):\n    bow = dictionary.doc2bow(text)\n    topics = lda_model.get_document_topics(bow)\n    yearly_topics[year].extend([t[0] for t in topics if t[1] > 0.3])\n\n# Визуализация трендов\nplt.figure(figsize=(15, 8))\nfor topic_id in range(15):\n    counts = [len([t for t in yearly_topics[year] if t == topic_id]) \n             for year in sorted(yearly_topics)]\n    plt.plot(sorted(yearly_topics), counts, label=f\"Topic {topic_id}\")\n\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:33:28.875359Z","iopub.execute_input":"2025-05-04T16:33:28.876084Z","iopub.status.idle":"2025-05-04T16:33:39.700430Z","shell.execute_reply.started":"2025-05-04T16:33:28.876055Z","shell.execute_reply":"2025-05-04T16:33:39.699662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Создание TF-IDF матрицы\ntfidf = TfidfVectorizer(max_features=1000)\nX = tfidf.fit_transform(df_filtered_copy['processed'].apply(' '.join))\n\n# Топ-20 значимых слов\nfeature_names = tfidf.get_feature_names_out()\ntfidf_scores = X.sum(axis=0).A1\ntop_words = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:50]\n\nprint(\"Топ-20 ключевых слов:\")\nfor word, score in top_words:\n    print(f\"{word}: {score:.2f}\")","metadata":{"id":"fchM7pHlLyIw","executionInfo":{"status":"ok","timestamp":1745958185065,"user_tz":-180,"elapsed":126,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"f2bc364f-9285-4142-c899-09622c9b4787","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T20:39:10.200530Z","iopub.execute_input":"2025-05-03T20:39:10.201204Z","iopub.status.idle":"2025-05-03T20:39:11.960035Z","shell.execute_reply.started":"2025-05-03T20:39:10.201173Z","shell.execute_reply":"2025-05-03T20:39:11.959312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install natasha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T20:39:46.962502Z","iopub.execute_input":"2025-05-03T20:39:46.962800Z","iopub.status.idle":"2025-05-03T20:39:55.865143Z","shell.execute_reply.started":"2025-05-03T20:39:46.962777Z","shell.execute_reply":"2025-05-03T20:39:55.864142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from natasha import (\n    Doc,\n    Segmenter,\n    MorphVocab,\n    NewsEmbedding,\n    NewsMorphTagger,\n    NewsNERTagger\n)\n\n# Инициализация компонентов\nsegmenter = Segmenter()\nmorph_vocab = MorphVocab()\nemb = NewsEmbedding()\nmorph_tagger = NewsMorphTagger(emb)\nner_tagger = NewsNERTagger(emb)\n\ndef extract_entities(text):\n    # Создаем объект Doc\n    doc = Doc(text)\n    \n    # Последовательная обработка\n    doc.segment(segmenter)       # Сегментация на токены\n    doc.tag_morph(morph_tagger)  # Морфологический разбор\n    doc.tag_ner(ner_tagger)      # Извлечение сущностей\n    \n    # Нормализация и сбор результатов\n    entities = []\n    for span in doc.spans:\n        span.normalize(morph_vocab)  # Приводим к нормальной форме\n        entities.append(\n            (span.normal, span.type)\n        )\n    \n    return entities\n\n# Применяем функцию к данным\ndf_filtered_copy['entities'] = df_filtered_copy['text'].apply(extract_entities)\n\n# Извлекаем все сущности\nall_entities = [ent for sublist in df_filtered_copy['entities'] for ent in sublist]\n\n# Фильтруем персоны и локации\npersons = [ent[0] for ent in all_entities if ent[1] == 'PER']\nlocations = [ent[0] for ent in all_entities if ent[1] == 'LOC']\n\nprint(\"Топ-10 персон:\", Counter(persons).most_common(10))\nprint(\"Топ-10 локаций:\", Counter(locations).most_common(10))","metadata":{"id":"zJOr_XJxOoQm","executionInfo":{"status":"ok","timestamp":1745958185258,"user_tz":-180,"elapsed":191,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"f25c6b85-1dd4-4a91-9f13-8882244f4d2a","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T20:59:05.794755Z","iopub.execute_input":"2025-05-03T20:59:05.795292Z","iopub.status.idle":"2025-05-03T21:08:19.618936Z","shell.execute_reply.started":"2025-05-03T20:59:05.795267Z","shell.execute_reply":"2025-05-03T21:08:19.618254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from razdel import sentenize\n\ndef extract_questions(text):\n    \"\"\"Извлекает предложения с вопросами из текста\"\"\"\n    return [\n        sentence.text.strip() \n        for sentence in sentenize(text) \n        if sentence.text.strip().endswith('?')\n    ]\n\n# Создаем список словарей с вопросами и исходными текстами\nresult = []\nfor _, row in df_filtered_copy.iterrows():\n    questions = extract_questions(row['text'])\n    for question in questions:\n        result.append({\n            'original_text': row['text'],\n            'question': question,\n            'year': row.get('year', None)  # если есть год\n        })\n\n# Создаем новый DataFrame\nquestions_df = pd.DataFrame(result)\n\n# Просмотр результатов\nprint(f\"Найдено вопросов: {len(questions_df)}\")\nquestions_df.head(1005) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:22:10.841319Z","iopub.execute_input":"2025-05-03T21:22:10.841590Z","iopub.status.idle":"2025-05-03T21:22:17.412997Z","shell.execute_reply.started":"2025-05-03T21:22:10.841571Z","shell.execute_reply":"2025-05-03T21:22:17.412197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Linguistic features extraction","metadata":{"id":"E2B96RWeR_DY"}},{"cell_type":"code","source":"df_ling = df.copy()","metadata":{"id":"jpbB_KCESw2F","executionInfo":{"status":"ok","timestamp":1746038037331,"user_tz":-180,"elapsed":20,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:24:43.711386Z","iopub.execute_input":"2025-05-12T09:24:43.711989Z","iopub.status.idle":"2025-05-12T09:24:43.728923Z","shell.execute_reply.started":"2025-05-12T09:24:43.711958Z","shell.execute_reply":"2025-05-12T09:24:43.728309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_ling[\"text\"] = df_ling[\"text\"].str.replace(r'\\n|\\t|\\r|</p>|<p>', ' ', regex=True)\n\ndf_ling\n","metadata":{"id":"XE_d3NxIXlb2","executionInfo":{"status":"ok","timestamp":1746038261388,"user_tz":-180,"elapsed":1359,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"d48b4bf0-4aa6-4716-c953-1358632a6af7","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:24:45.854034Z","iopub.execute_input":"2025-05-12T09:24:45.854326Z","iopub.status.idle":"2025-05-12T09:24:46.123968Z","shell.execute_reply.started":"2025-05-12T09:24:45.854306Z","shell.execute_reply":"2025-05-12T09:24:46.123325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Словарь для хранения DataFrame\ndecade_dfs = {}\n\n# Группировка и сохранение\nfor decade, group in df_ling.groupby('decade'):\n    decade_dfs[decade] = group[[\"text\", \"year\"]].copy()\n\n    # Вывод информации\n    print(f\"\\nДекада {decade}-{decade+9} ({len(decade_dfs[decade])} записей)\")","metadata":{"id":"KS4g70naaDZ7","executionInfo":{"status":"ok","timestamp":1746038277264,"user_tz":-180,"elapsed":40,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"63d8e709-11b4-4fc1-e26c-49a4e62e2e84","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:24:48.241362Z","iopub.execute_input":"2025-05-12T09:24:48.241681Z","iopub.status.idle":"2025-05-12T09:24:48.256271Z","shell.execute_reply.started":"2025-05-12T09:24:48.241661Z","shell.execute_reply":"2025-05-12T09:24:48.255572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decade_dfs[1900]","metadata":{"id":"g57ul63qaPJH","executionInfo":{"status":"ok","timestamp":1746038279976,"user_tz":-180,"elapsed":69,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"c1200405-6131-4fa8-bd5e-420d074cdba3","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T20:46:00.812554Z","iopub.execute_input":"2025-05-11T20:46:00.813084Z","iopub.status.idle":"2025-05-11T20:46:00.820743Z","shell.execute_reply.started":"2025-05-11T20:46:00.813064Z","shell.execute_reply":"2025-05-11T20:46:00.820076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m spacy download ru_core_news_lg","metadata":{"id":"Lr5FYDGfeH2l","executionInfo":{"status":"ok","timestamp":1746038199313,"user_tz":-180,"elapsed":32511,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"681a93b7-3504-4b06-d467-53d48ac8075c","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:24:53.039395Z","iopub.execute_input":"2025-05-12T09:24:53.040072Z","iopub.status.idle":"2025-05-12T09:25:13.085260Z","shell.execute_reply.started":"2025-05-12T09:24:53.040048Z","shell.execute_reply":"2025-05-12T09:25:13.084302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install stanza","metadata":{"id":"RdvsfSDheTVE","executionInfo":{"status":"ok","timestamp":1746038393782,"user_tz":-180,"elapsed":109312,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"6a4ba754-a506-4c4a-c9e2-6584f010d287","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:25:13.086970Z","iopub.execute_input":"2025-05-12T09:25:13.087247Z","iopub.status.idle":"2025-05-12T09:26:27.954220Z","shell.execute_reply.started":"2025-05-12T09:25:13.087225Z","shell.execute_reply":"2025-05-12T09:26:27.953176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc --version\n!pip install cupy-cuda12x","metadata":{"id":"2ZxevPYCLGmT","executionInfo":{"status":"ok","timestamp":1746038397527,"user_tz":-180,"elapsed":3743,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"5d8513cd-c92d-4b0b-ec97-79effca1327d","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:26:27.956009Z","iopub.execute_input":"2025-05-12T09:26:27.956396Z","iopub.status.idle":"2025-05-12T09:26:31.260737Z","shell.execute_reply.started":"2025-05-12T09:26:27.956362Z","shell.execute_reply":"2025-05-12T09:26:31.259811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import stanza\nimport spacy\nimport cupy\n\n# Инициализация моделей\nif cupy.is_available():\n    print(\"GPU доступен\")\n    spacy.prefer_gpu()\nnlp_spacy = spacy.load(\"ru_core_news_lg\")\nstanza.download('ru')\nnlp_stanza = stanza.Pipeline('ru', use_gpu=True)\n","metadata":{"id":"M5DeFVMawbHb","executionInfo":{"status":"ok","timestamp":1746038447270,"user_tz":-180,"elapsed":49742,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"8c23de64-fa7c-443a-83ea-169cbeab085a","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:29:02.697606Z","iopub.execute_input":"2025-05-12T09:29:02.697901Z","iopub.status.idle":"2025-05-12T09:29:35.135591Z","shell.execute_reply.started":"2025-05-12T09:29:02.697881Z","shell.execute_reply":"2025-05-12T09:29:35.134976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indef_list = {'некто', 'нечто', 'некоторый', 'несколько', 'некий', 'кое-кто', \"кое-что\", \"кое-какой\", \"кое-чей\", \"кто-то\", \"что-то\", \"какой-то\", \"чей-то\", \"кто-нибудь\", \"что-нибудь\", \"какой-нибудь\", \"чей-нибудь\", \"кто-либо\", \"что-либо\", \"какой-либо\", \"чей-либо\"}\n\nplace_adv_list = {'вблизи',\n                  'вверху',\n                  'вдалеке',\n                  'вдали',\n                  'взаперти',\n                  'вне',\n                  'внизу',\n                  'внутри',\n                  'вовне',\n                  'возле',\n                  'вокруг',\n                  'впереди',\n                  'всюду',\n                  'высоко',\n                  'где',\n                  'далеко',\n                  'далёко',\n                  'изнутри',\n                  'навстречу',\n                  'наособицу',\n                  'невдалеке',\n                  'недалеко',\n                  'недалечко',\n                  'неподалёку',\n                  'низом',\n                  'одаль',\n                  'одесную',\n                  'около',\n                  'окрест',\n                  'откуда',\n                  'отсюда',\n                  'передом',\n                  'поблизости',\n                  'повсюду',\n                  'поодаль',\n                  'посередине',\n                  'посерёдке',\n                  'посреди',\n                  'посредине',\n                  'прочь',\n                  'рядом',\n                  'сверху',\n                  'сзади',\n                  'слева',\n                  'снаружи',\n                  'снизу',\n                  'спереди',\n                  'справа',\n                  'там',\n                  'вверх',\n                  'вниз',\n                  'доселе',\n                  'досель',\n                  'дотуда',\n                  'дотудова',\n                  'изовсюду',\n                  'кое-куда',\n                  'кое-откуда',\n                  'кой-куда',\n                  'куда',\n                  'куда угодно',\n                  'куда-либо',\n                  'куда-нибудь',\n                  'куда-то',\n                  'никуда',\n                  'нигде',\n                  'отколь',\n                  'откуда',\n                  'откуда-либо',\n                  'откуда-то',\n                  'откудова',\n                  'отовсюду',\n                  'отселе',\n                  'отсель',\n                  'отсюда',\n                  'отсюдова',\n                  'отсюду',\n                  'оттелева',\n                  'оттель',\n                  'оттоль',\n                  'оттуда',\n                  'оттудова',\n                  'сюда',\n                  'туда',\n                  'тут',\n                  'туда-обратно',\n                  'туда-сюда'\n                  }\n\ntime_adv_list = {\n'анадысь',\n'ввек',\n'вдалеке',\n'вдали',\n'весной',\n'весною',\n'вечером',\n'вечор',\n'вовремя',\n'впоследствии',\n'впредь',\n'встарь',\n'вчера',\n'вчерась',\n'давеча',\n'давно',\n'дальше',\n'днём',\n'днесь',\n'днями',\n'доднесь',\n'долго',\n'доле',\n'долее',\n'доныне',\n'досветла',\n'дотемна',\n'древле',\n'ежедневно',\n'ежеквартально',\n'ежемесячно',\n'еженочно',\n'еженощно',\n'заблаговременно',\n'завременно',\n'завтра',\n'задолго',\n'зараз',\n'заранее',\n'засветло',\n'засим',\n'затем',\n'затемно',\n'зимой',\n'зимою',\n'издревле',\n'иногда',\n'испокон',\n'каждодневно',\n'каждомесячно',\n'когда-либо',\n'летом',\n'навсегда',\n'надолго',\n'надысь',\n'накануне',\n'намедни',\n'насовсем',\n'наутро',\n'невовремя',\n'недавно',\n'незадолго',\n'несвоевременно',\n'нонеча',\n'нонче',\n'ночию',\n'ночью',\n'ныне',\n'нынече',\n'нынче',\n'однажды',\n'отныне'\n'первоначально',\n'поднесь',\n'подоле',\n'подчас',\n'позднее',\n'поздно',\n'позже',\n'пока',\n'покамест',\n'покуда',\n'поначалу',\n'поныне',\n'порой',\n'после'\n'послезавтра',\n'прежде',\n'ранее',\n'рано',\n'раньше',\n'редко',\n'сегодня',\n'сейгод',\n'сейчас',\n'скоро',\n'смальства',\n'смолоду',\n'сперва',\n'спокон',\n'сразу',\n'стемна',\n'сыздетства',\n'сызмала',\n'сызмалу',\n'сызмальства',\n'теперича',\n'теперь',\n'третёвось',\n'третьёвось',\n'утром',\n'часто',\n'ща',\n'щас'\n}","metadata":{"id":"Ucpgw9ox77Fv","executionInfo":{"status":"ok","timestamp":1746038520877,"user_tz":-180,"elapsed":68,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:29:43.196146Z","iopub.execute_input":"2025-05-12T09:29:43.196757Z","iopub.status.idle":"2025-05-12T09:29:43.207691Z","shell.execute_reply.started":"2025-05-12T09:29:43.196732Z","shell.execute_reply":"2025-05-12T09:29:43.206728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Загрузка данных \nabstraction_df_noun = pd.read_csv('/kaggle/input/vkr-data/Slovar.r.ya..s.indeksom.konkretnosti.slov.csv')\nabstraction_df_adj = pd.read_csv ('/kaggle/input/vkr-data/Slovar.r.ya..s.indeksom.konkretnosti.slov_.csv')\n","metadata":{"id":"PDenNn4aP5cg","executionInfo":{"status":"ok","timestamp":1746038525166,"user_tz":-180,"elapsed":92,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:26:34.545565Z","iopub.execute_input":"2025-05-12T09:26:34.546086Z","iopub.status.idle":"2025-05-12T09:26:34.691835Z","shell.execute_reply.started":"2025-05-12T09:26:34.546063Z","shell.execute_reply":"2025-05-12T09:26:34.691177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"abstraction_df_noun","metadata":{"id":"FotdEgvvV-YM","executionInfo":{"status":"ok","timestamp":1746038527271,"user_tz":-180,"elapsed":89,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"52e88b52-80ec-4aef-e6e1-48ef12812bad","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:33:44.992160Z","iopub.execute_input":"2025-05-01T07:33:44.992473Z","iopub.status.idle":"2025-05-01T07:33:45.004410Z","shell.execute_reply.started":"2025-05-01T07:33:44.992452Z","shell.execute_reply":"2025-05-01T07:33:45.003765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"abstraction_df_adj","metadata":{"id":"2cGRDjLQmuKC","executionInfo":{"status":"ok","timestamp":1746038530033,"user_tz":-180,"elapsed":72,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"2ee2c4a0-06cf-4b9b-fef1-fd247522095f","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:33:47.473513Z","iopub.execute_input":"2025-05-01T07:33:47.473820Z","iopub.status.idle":"2025-05-01T07:33:47.484269Z","shell.execute_reply.started":"2025-05-01T07:33:47.473796Z","shell.execute_reply":"2025-05-01T07:33:47.483399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_noun(word):\n    \"\"\"Нормализация существительных с обработкой исключений\"\"\"\n    try:\n        parsed = morph.parse(str(word))[0]\n        if 'NOUN' in parsed.tag:\n            return parsed.normal_form.lower().strip()\n        return str(word).lower().strip()\n    except:\n        return str(word).lower().strip()\n\ndef normalize_adj(word):\n    \"\"\"Нормализация прилагательных с обработкой исключений\"\"\"\n    try:\n        parsed = morph.parse(str(word))[0]\n        if 'ADJF' in parsed.tag:\n            return parsed.normal_form.lower().strip()\n        return str(word).lower().strip()\n    except:\n        return str(word).lower().strip()\n\n# Обработка существительных\nnormalized_dict_noun = defaultdict(list)\n\nfor raw_word, score in zip(abstraction_df_noun['word'], abstraction_df_noun['Индекс С/A']):\n    normalized = normalize_noun(raw_word)\n    normalized_dict_noun[normalized].append(float(score))  # Конвертация в float\n\nabstraction_dict_noun = {}\nfor lemma, scores in normalized_dict_noun.items():\n    abstraction_dict_noun[lemma] = sum(scores) / len(scores)  # Ручной расчет среднего\n\nprint({k: v for k, v in list(abstraction_dict_noun.items())[:5]})\n\n# Обработка прилагательных\nabstraction_df_adj = abstraction_df_adj.copy()\nabstraction_df_adj['Индекс С/A'] = abstraction_df_adj['Индекс С/A'].astype(float)\nabstraction_df_adj['normalized'] = abstraction_df_adj['w'].apply(normalize_adj)\n\nabstraction_dict_adj = (\n    abstraction_df_adj\n    .groupby('normalized')['Индекс С/A']\n    .mean()\n    .to_dict()\n)\n\nprint({k: float(v) for k, v in list(abstraction_dict_adj.items())[:5]})\n","metadata":{"id":"Dg366vJnRWhc","executionInfo":{"status":"ok","timestamp":1746038539731,"user_tz":-180,"elapsed":7185,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"813587d4-ff09-4248-f399-5d223a1526f2","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:26:38.108430Z","iopub.execute_input":"2025-05-12T09:26:38.109146Z","iopub.status.idle":"2025-05-12T09:26:44.855294Z","shell.execute_reply.started":"2025-05-12T09:26:38.109122Z","shell.execute_reply":"2025-05-12T09:26:44.854536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n# Предварительная настройка\ndim_patterns = [re.compile(rf'({s})[а-я]*$') for s in [\n    'ик', 'ек', 'к', 'ок', 'ёк', 'ец', 'иц',\n    'очк', 'ечк', 'оньк', 'еньк', 'ышк', 'ишк', 'ушк', 'юшк'\n]]","metadata":{"id":"6VhZipy-qh8S","executionInfo":{"status":"ok","timestamp":1746038562669,"user_tz":-180,"elapsed":3,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:27:01.096454Z","iopub.execute_input":"2025-05-12T09:27:01.096980Z","iopub.status.idle":"2025-05-12T09:27:01.104161Z","shell.execute_reply.started":"2025-05-12T09:27:01.096950Z","shell.execute_reply":"2025-05-12T09:27:01.103240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import deque\nclass RussianTextAnalyzer:\n    def __init__(self, text):\n        self.text = text\n        self.text_len = len(text)\n        self.spacy_doc = nlp_spacy(text)\n        self.stanza_doc = nlp_stanza(text)\n        self.words = [token.text for token in self.spacy_doc if not token.is_punct]\n        self.abstraction_dict_noun = {k: float(v) for k, v in abstraction_dict_noun.items()}\n        self.abstraction_dict_adj = {k: float(v) for k, v in abstraction_dict_adj.items()}\n\n        # Инициализируем счетчики\n        self.features = {}\n\n    def analyze(self):\n        self._lexical_features()\n        self._syntactic_features()\n        self._morphological_features()\n        return self.features\n\n    def _lexical_features(self):\n        # Лексические признаки\n        pos_counts = Counter(token.pos_ for token in self.spacy_doc)\n        # morph_features_count = Counter(token.morph.to_dict() for token in self.spacy_doc)\n\n        gram_spacy = {}\n        for token in self.spacy_doc:\n          if token.pos_ not in gram_spacy:\n            gram_spacy[token.pos_] = Counter(token.morph.to_dict().values())\n          else:\n            gram_spacy[token.pos_] += Counter(token.morph.to_dict().values())\n\n        gram_stanza = {}\n        for sent in self.stanza_doc.sentences:\n          for word in sent.words:\n            if word.upos not in gram_stanza:\n              gram_stanza[word.upos] = Counter(dict(item.split('=') for item in word.feats.split('|')).values() if word.feats else {})\n            else:\n              gram_stanza[word.upos] += Counter(dict(item.split('=') for item in word.feats.split('|')).values() if word.feats else {})\n\n\n\n        def analyze_abstr():\n            upos_lemmas = [\n                (word.upos, word.lemma.lower().strip())\n                for sent in self.stanza_doc.sentences\n                for word in sent.words\n            ]\n\n            scores_noun = []\n            scores_adj = []\n            total_nouns = 0\n            total_adj = 0\n\n            for upos, lemma in upos_lemmas:\n                if upos == 'NOUN':\n                    total_nouns += 1\n                    if lemma in self.abstraction_dict_noun:\n                        scores_noun.append(float(self.abstraction_dict_noun[lemma]))  # Явное преобразование\n                if upos == 'ADJ':\n                    total_adj += 1\n                    if lemma in self.abstraction_dict_adj:\n                        scores_adj.append(float(self.abstraction_dict_adj[lemma]))\n\n            # Расчет статистики\n            mean_noun = sum(scores_noun)/len(scores_noun) if scores_noun else None\n            mean_adj = sum(scores_adj)/len(scores_adj) if scores_adj else None\n            sorted_scores_noun = sorted(scores_noun)\n            sorted_scores_adj = sorted(scores_adj)\n            n_noun = len(sorted_scores_noun)\n            n_adj = len(sorted_scores_adj)\n            median_noun = (\n                sorted_scores_noun[n_noun//2]\n                if n_noun % 2 else\n                (sorted_scores_noun[n_noun//2-1] + sorted_scores_noun[n_noun//2])/2\n            ) if scores_noun else None\n            median_adj = (\n                sorted_scores_adj[n_adj//2]\n                if n_adj % 2 else\n                (sorted_scores_adj[n_adj//2-1] + sorted_scores_adj[n_adj//2])/2\n            ) if scores_adj else None\n\n            return {\n                'mean_score_noun': mean_noun,\n                'mean_score_adj': mean_adj,\n                'median_score_noun': median_noun,\n                'median_score_adj': median_adj,\n                'coverage_noun': len(scores_noun)/total_nouns if total_nouns else 0.0,\n                'coverage_adj': len(scores_adj)/total_adj if total_adj else 0.0,\n                'total_nouns': total_nouns,\n                'total_adj': total_adj\n            }\n\n        abstr_pos = analyze_abstr()\n        self.features.update({\n            'first_person_pronouns_sing': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=First' in t.morph\n                                                and 'Number=Sing' in t.morph\n                                              ),\n            'first_person_pronouns_plur': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=First' in t.morph\n                                                and 'Number=Plur' in t.morph\n                                              ),\n            'second_person_pronouns_sing': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=Second' in t.morph\n                                                and 'Number=Sing' in t.morph\n                                              ),\n            'second_person_pronouns_plur': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=Second' in t.morph\n                                                and 'Number=Plur' in t.morph\n                                              ),\n            'third_person_pronouns_masc': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=Third' in t.morph\n                                                and 'Gender=Masc' in t.morph\n                                              ),\n            'third_person_pronouns_fem': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=Third' in t.morph\n                                                and 'Gender=Fem' in t.morph\n                                            ),\n            'third_person_pronouns_neut': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=Third' in t.morph\n                                                and 'Gender=Neut' in t.morph\n                                             ),\n            'third_person_pronouns_plur': sum(\n                                                1\n                                                for t in self.spacy_doc\n                                                if t.pos_ == 'PRON'\n                                                and 'Person=Third' in t.morph\n                                                and 'Number=Plur' in t.morph\n                                             ),\n            'demonstrative_pronouns': gram_stanza.get('DET', Counter()).get('Dem',0),\n            'prepositions': pos_counts.get('ADP', 0),\n            'coordinationg_conjunctions': pos_counts.get('CCONJ', 0),\n            'indefinite_pronouns': sum(\n                                        1\n                                        for sent in self.stanza_doc.sentences\n                                        for word in sent.words\n                                        if word.lemma.lower() in indef_list\n                                        and word.upos in {'DET', 'PRON'}\n                                    ),\n            'place_adverbials': sum(\n                                        1\n                                        for sent in self.stanza_doc.sentences\n                                        for word in sent.words\n                                        if word.lemma.lower() in place_adv_list\n                                        and word.upos in {'ADV'}\n                                    ),\n            'time_adverbials': sum(\n                                        1\n                                        for sent in self.stanza_doc.sentences\n                                        for word in sent.words\n                                        if word.lemma.lower() in time_adv_list\n                                        and word.upos in {'ADV'}\n                                    ),\n            'noun_anim': sum(\n                                  1\n                                  for t in self.spacy_doc\n                                  if t.pos_ == 'NOUN'\n                                  and 'Animacy=Anim' in t.morph\n                                ),\n            'noun_inan': sum(\n                              1\n                              for t in self.spacy_doc\n                              if t.pos_ == 'NOUN'\n                              and 'Animacy=Inan' in t.morph\n                            ),\n            'noun_abstr_index': abstr_pos['mean_score_noun'],\n            'adj_abstr_index': abstr_pos['mean_score_adj'],\n            'latin_letters': sum(1\n                                    for token in self.spacy_doc\n                                    if any('LATN' in parse.tag for parse in morph.parse(token.text))\n                                  ),\n            'propr_name': sum(1\n                                    for token in self.spacy_doc\n                                    if any('Name' in parse.tag for parse in morph.parse(token.text))\n                                  ),\n            'patr_name': sum(1\n                                    for token in self.spacy_doc\n                                    if any('Patr' in parse.tag for parse in morph.parse(token.text))\n                                  ),\n            'sur_name': sum(1\n                                    for token in self.spacy_doc\n                                    if any('Surn' in parse.tag for parse in morph.parse(token.text))\n                                  ),\n            'praedicative': sum(1\n                                    for token in self.spacy_doc\n                                    if any('PRED' in parse.tag for parse in morph.parse(token.text)) or any('Prdx' in parse.tag for parse in morph.parse(token.text))\n                                  ),\n            'geo_name': sum(\n                              1\n                              for token in self.spacy_doc\n                              if token.pos_ == \"PROPN\"\n                              if any('Geox' in parse.tag for parse in morph.parse(token.text))\n                              ),\n            'intj': sum(1\n                                    for token in self.spacy_doc\n                                    if any('INTJ' in parse.tag for parse in morph.parse(token.text))\n                                  )\n        })\n\n\n    def _syntactic_features(self):\n        # Синтаксические признаки\n        sentence_lengths = [len(sent.text.split()) for sent in self.spacy_doc.sents]\n\n        def is_minor_constituent(word, sentence):\n            \"\"\"Определяет тип сочинительной конструкции\"\"\"\n            if word.head == 0:\n                return None\n\n            head = sentence.words[word.head - 1]\n\n            # Определяем тип конструкции\n            if word.deprel == 'conj':\n                if head.upos == 'NOUN' and word.upos == 'NOUN':\n                    return 'NOUN'\n                elif head.upos == 'ADJ' and word.upos == 'ADJ':\n                    return 'ADJ'\n                elif head.upos == 'VERB' and word.upos == 'VERB':\n                    return 'VERB'\n                elif head.upos == 'ADV' and word.upos == 'ADV':\n                    return 'ADV'\n                elif head.deprel in {'obj', 'nsubj', 'nmod'}:\n                    return 'NOUN'  # Для именных дополнений\n                elif head.deprel == 'amod':\n                    return 'ADJ'   # Для определений\n                elif head.deprel == 'advmod':\n                    return 'ADV'   # Для обстоятельств\n\n            return None\n\n        def extract_minor_coordinations(text):\n            doc = self.stanza_doc\n            coordination_counts = {\n                'NOUN': 0,\n                'ADJ': 0,\n                'VERB': 0,\n                'ADV': 0,\n                'OTHER': 0\n            }\n\n            for sentence in doc.sentences:\n                for word in sentence.words:\n                    if word.deprel == 'conj':\n                        const_type = is_minor_constituent(word, sentence)\n                        if const_type:\n                            if const_type in coordination_counts:\n                                coordination_counts[const_type] += 1\n                            else:\n                                coordination_counts['OTHER'] += 1\n\n            return coordination_counts\n\n        coordination_counts = extract_minor_coordinations(text)\n\n        def analyze_tree(text):\n            doc = self.stanza_doc\n            features = {\n                'max_tree_depth': 0,\n                'avg_np_length': 0.0,\n                'avg_vp_length': 0.0,\n                'inversion_count': 0,\n                'ellipsis_count': 0\n            }\n\n            all_depths = []\n            np_lengths = []\n            vp_lengths = []\n\n            for sentence in doc.sentences:\n                # 2.1. Глубина дерева\n                depths = _calculate_depths(sentence)\n                all_depths.extend(depths)\n\n                # 2.2. Длина NP/VP\n                nps = _extract_phrases(sentence, 'NP')\n                vps = _extract_phrases(sentence, 'VP')\n                np_lengths.extend([len(np) for np in nps])\n                vp_lengths.extend([len(vp) for vp in vps])\n\n                # 2.3. Инверсии\n                features['inversion_count'] += _count_inversions(sentence)\n\n                # 2.4. Эллипсис\n                features['ellipsis_count'] += sum(1 for word in sentence.words\n                                                if word.deprel == 'orphan')\n\n            # Расчет итоговых значений\n            if all_depths:\n                features['max_tree_depth'] = max(all_depths)\n            if np_lengths:\n                features['avg_np_length'] = sum(np_lengths)/len(np_lengths)\n            if vp_lengths:\n                features['avg_vp_length'] = sum(vp_lengths)/len(vp_lengths)\n\n            return features\n\n        def _calculate_depths(sentence):\n            \"\"\"Улучшенный расчет глубины дерева с использованием BFS\"\"\"\n            depths = []\n            root = next((word for word in sentence.words if word.head == 0), None)\n            if not root:\n                return []\n\n            queue = deque([(root, 0)])\n            visited = set()\n\n            while queue:\n                word, depth = queue.popleft()\n                if word.id in visited:\n                    continue\n                visited.add(word.id)\n                depths.append(depth)\n\n                # Добавляем дочерние узлы\n                children = [w for w in sentence.words if w.head == word.id]\n                for child in children:\n                    queue.append((child, depth + 1))\n\n            return depths\n\n        def _extract_phrases(sentence, phrase_type):\n            \"\"\"Улучшенное извлечение фраз с фильтрацией\"\"\"\n            phrases = []\n            targets = {\n                'NP': ['NOUN', 'PROPN', 'PRON'],\n                'VP': ['VERB', 'AUX']\n            }\n\n            for word in sentence.words:\n                if word.upos in targets[phrase_type]:\n                    phrase = _get_phrase(sentence, word.id)\n                    if _is_valid_phrase(phrase_type, sentence, phrase):\n                        phrases.append(phrase)\n\n            return phrases\n\n        def _get_phrase(sentence, head_id):\n            \"\"\"Поиск в ширину для более точного определения границ фразы\"\"\"\n            phrase = []\n            queue = deque([head_id])\n            visited = set()\n\n            while queue:\n                current_id = queue.popleft()\n                if current_id in visited:\n                    continue\n                visited.add(current_id)\n\n                phrase.append(current_id)\n                current_word = sentence.words[current_id-1]\n\n                # Добавляем только непосредственные зависимые\n                children = [w.id for w in sentence.words\n                          if w.head == current_id\n                          and w.deprel not in ['punct', 'cc', 'mark']]\n                queue.extend(children)\n\n            return sorted(phrase)\n\n        def _is_valid_phrase(phrase_type, sentence, phrase_ids):\n            \"\"\"Проверка валидности извлеченной фразы\"\"\"\n            if len(phrase_ids) < 1:\n                return False\n\n            main_word = sentence.words[phrase_ids[0]-1]\n\n            if phrase_type == 'NP':\n                return main_word.upos in ['NOUN', 'PROPN', 'PRON']\n            elif phrase_type == 'VP':\n                return main_word.upos in ['VERB', 'AUX']\n            return False\n\n        def _count_inversions(sentence):\n            \"\"\"Улучшенный подсчет инверсий\"\"\"\n            inversions = 0\n            for word in sentence.words:\n                if word.deprel == 'nsubj':\n                    verb = sentence.words[word.head-1]\n                    # Более гибкое условие для русского языка\n                    if word.id > verb.id and (word.id - verb.id) >= 1:\n                        context = sentence.words[verb.id-1:word.id]\n                        if not any(w.deprel == 'advmod' for w in context):\n                            inversions += 1\n            return inversions\n\n        tree_analysis = analyze_tree(text)\n\n        def count_syllables_ru(word):\n            \"\"\"Улучшенный подсчет слогов для русского языка\"\"\"\n            vowels = 'аеёиоуыэюя'\n            word = word.lower()\n            count = 0\n            prev_vowel = False\n\n            for char in word:\n                if char in vowels:\n                    if not prev_vowel:  # Учитываем только последовательные гласные как один слог\n                        count += 1\n                    prev_vowel = True\n                else:\n                    prev_vowel = False\n\n            # Гарантируем минимум 1 слог для коротких слов\n            return max(1, count)\n\n        def flesch_kincaid_russian(text):\n            \n            try:\n\n                # Сбор статистики\n                sentences = self.stanza_doc.sentences\n                num_sentences = len(sentences)\n                words = [word.text for sent in sentences for word in sent.words]\n                num_words = len(words)\n\n                if num_sentences == 0 or num_words == 0:\n                    return 0.0\n\n                # Подсчет слогов\n                total_syllables = sum(count_syllables_ru(word) for word in words)\n\n                # Расчет показателей\n                ASL = num_words / num_sentences  # Average Sentence Length\n                ASW = total_syllables / num_words  # Average Syllables per Word\n\n                # Применение формулы\n                score = 206.835 - 1.52 * ASL - 65.14 * ASW\n\n                # Ограничение диапазона 0-100\n                return max(0, min(100, round(score, 2)))\n\n            except Exception as e:\n                print(f\"Ошибка при обработке текста: {e}\")\n                return 0.0\n\n        flesch_kincaid_index = flesch_kincaid_russian(self.text)\n\n\n        self.features.update({\n            'mean_sentence_length': sum(sentence_lengths)/len(sentence_lengths) if sentence_lengths else 0,\n            'subordinate_clauses': sum(1 for token in self.spacy_doc if token.dep_ == 'mark' or token.pos_ == 'SCONJ'),\n            'type-token ratio': len(set(self.words)) / len(self.words) * 100 if self.words else 0,\n            'word length': sum(len(word) for word in self.words) / len(self.words),\n            'noun_coordination': coordination_counts['NOUN'],\n            'adj_coordination': coordination_counts['ADJ'],\n            'verb_coordination': coordination_counts['VERB'],\n            'adv_coordination': coordination_counts['ADV'],\n            'other_coordination': coordination_counts['OTHER'],\n            'max_tree_depth': tree_analysis['max_tree_depth'],\n            'avg_np_length': tree_analysis['avg_np_length'],\n            'avg_vp_length': tree_analysis['avg_vp_length'],\n            'inversion_count': tree_analysis['inversion_count'],\n            'ellipsis_count': tree_analysis['ellipsis_count'],\n            'flesch_kincaid_index': flesch_kincaid_index\n        })\n\n    def _morphological_features(self):\n        # Морфологические признаки\n\n        self.features.update({\n            'perfect_aspect': sum(1 for token in self.spacy_doc if  token.pos_ == 'VERB' and 'Aspect=Perf' in token.morph),\n            'imperfect_aspect': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Aspect=Imp' in token.morph),\n            'past_tense': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Tense=Past' in token.morph),\n            'present_tense': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Tense=Pres' in token.morph),\n            'fut_tense': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Tense=Fut' in token.morph),\n            'ind_mood_verb': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Mood=Ind' in token.morph),\n            'imp_mood_verb': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Mood=Imp' in token.morph),\n            'cnd_mood_verb': sum(1 for token in self.spacy_doc if token.pos_ == 'AUX' and 'Mood=Cnd' in token.morph),\n            'gerunds': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'VerbForm=Conv' in token.morph),\n            'participles': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'VerbForm=Part' in token.morph),\n            'infinitives': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'VerbForm=Inf' in token.morph),\n            'finite_verbs': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'VerbForm=Fin' in token.morph),\n            'passive_voice': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Voice=Pass' in token.morph),\n            'active_voice': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Voice=Act' in token.morph),\n            'middle_voice': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and 'Voice=Mid' in token.morph),\n            'neg_polarity': sum(1 for token in self.spacy_doc if token.pos_ == 'PART' and 'Polarity=Neg' in token.morph),\n            'first_pers_verb_sing': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and ('Number=Sing' and 'Person=First') in token.morph),\n            'second_pers_verb_sing': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and ('Number=Sing' and 'Person=Second') in token.morph),\n            'third_pers_verb_sing': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and ('Number=Sing' and 'Person=Third') in token.morph),\n            'first_pers_verb_plur': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and ('Number=Plur' and 'Person=First') in token.morph),\n            'second_pers_verb_plur': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and ('Number=Plur' and 'Person=Second') in token.morph),\n            'third_pers_verb_plur': sum(1 for token in self.spacy_doc if token.pos_ == 'VERB' and ('Number=Plur' and 'Person=Third') in token.morph),\n            'trans_verb': sum(\n                              1\n                              for token in self.spacy_doc\n                              if token.pos_ == \"VERB\"\n                              if any('tran' in parse.tag for parse in morph.parse(token.text))\n                              ),\n            'intr_verb': sum(\n                              1\n                              for token in self.spacy_doc\n                              if token.pos_ == \"VERB\"\n                              if any('intr' in parse.tag for parse in morph.parse(token.text))\n                              ),\n            'not_inv_verb': sum(\n                              1\n                              for token in self.spacy_doc\n                              if token.pos_ == \"VERB\"\n                              if any('excl' in parse.tag for parse in morph.parse(token.text))\n                              ),\n            'sing_noun': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Number=Sing' in token.morph),\n            'plur_noun': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Number=Plur' in token.morph),\n            'plr_tantum_noun': sum(\n                                    1\n                                    for token in self.spacy_doc\n                                    if token.pos_ == \"NOUN\"\n                                    if any('Pltm' in parse.tag for parse in morph.parse(token.text))\n                                   ),\n            'sing_tantum_noun': sum(\n                                    1\n                                    for token in self.spacy_doc\n                                    if token.pos_ == \"NOUN\"\n                                    if any('Sgtm' in parse.tag for parse in morph.parse(token.text))\n                                   ),\n            'noun_fem': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Gender=Fem' in token.morph),\n            'noun_masc': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Gender=Masc' in token.morph),\n            'noun_neut': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Gender=Neut' in token.morph),\n            'noun_case_nom': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Case=Nom' in token.morph),\n            'noun_case_gen': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Case=Gen' in token.morph),\n            'noun_case_dat': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Case=Dat' in token.morph),\n            'noun_case_acc': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Case=Acc' in token.morph),\n            'noun_case_loc': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Case=Loc' in token.morph),\n            'noun_case_ins': sum(1 for token in self.spacy_doc if token.pos_ == 'NOUN' and 'Case=Ins' in token.morph),\n            'noun_case_voc': sum(\n                                    1\n                                    for token in self.spacy_doc\n                                    if token.pos_ == \"NOUN\"\n                                    if any('voct' in parse.tag for parse in morph.parse(token.text))\n                                ),\n            'fixed_noun': sum(\n                                    1\n                                    for token in self.spacy_doc\n                                    if token.pos_ == \"NOUN\"\n                                    if any('Fixd' in parse.tag for parse in morph.parse(token.text))\n                                ),\n            'sing_adj': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Number=Sing' in token.morph),\n            'plur_adj': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Number=Plur' in token.morph),\n            'adj_fem': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Gender=Fem' in token.morph),\n            'adj_masc': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Gender=Masc' in token.morph),\n            'adj_neut': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Gender=Neut' in token.morph),\n            'adj_case_nom': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Case=Nom' in token.morph),\n            'adj_case_gen': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Case=Gen' in token.morph),\n            'adj_case_dat': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Case=Dat' in token.morph),\n            'adj_case_acc': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Case=Acc' in token.morph),\n            'adj_case_loc': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Case=Loc' in token.morph),\n            'adj_case_ins': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Case=Ins' in token.morph),\n            'adj_case_voc': sum(\n                                    1\n                                    for token in self.spacy_doc\n                                    if token.pos_ == \"ADJ\"\n                                    if any('voct' in parse.tag for parse in morph.parse(token.text))\n                                ),\n            'adj_degree_pos': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Degree=Pos' in token.morph),\n            'adj_degree_comp': sum(1 for token in self.spacy_doc if token.pos_ == 'ADJ' and 'Degree=Cmp' in token.morph),\n            'adj_degree_super': sum(1\n                                    for token in self.spacy_doc\n                                    if token.pos_ == 'ADJ'\n                                    if any('Supr' in parse.tag for parse in morph.parse(token.text))\n                                  ),\n            'full_adj': sum(1 for token in self.spacy_doc if any('ADJF' in parse.tag for parse in morph.parse(token.text))),\n            'shrot_adj': sum(1 for token in self.spacy_doc if any('ADJS' in parse.tag for parse in morph.parse(token.text)) and 'StyleVariant=Short' in token.morph),\n            'adv_degree_pos': sum(1 for token in self.spacy_doc if token.pos_ == 'ADV' and 'Degree=Pos' in token.morph),\n            'adv_degree_comp': sum(1 for token in self.spacy_doc if token.pos_ == 'ADV' and 'Degree=Cmp' in token.morph),\n            'quant_num': sum(1 for token in self.spacy_doc if token.pos_ == 'NUM'),\n            'anum_num': sum(1\n                                    for token in self.spacy_doc\n                                    if token.pos_ == 'ADJ'\n                                    if any('Anum' in parse.tag for parse in morph.parse(token.text))\n                                  ),\n            'dim_nouns': sum(1 for t in self.spacy_doc  if t.pos_ == 'NOUN' and any(p.search(t.lemma_.lower()) for p in dim_patterns)),\n            'dim_adj': sum(1 for t in self.spacy_doc  if t.pos_ == 'ADJ' and any(p.search(t.lemma_.lower()) for p in dim_patterns))\n        })","metadata":{"id":"rdml9j3tt985","executionInfo":{"status":"ok","timestamp":1746038566970,"user_tz":-180,"elapsed":120,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:29:52.566809Z","iopub.execute_input":"2025-05-12T09:29:52.567107Z","iopub.status.idle":"2025-05-12T09:29:52.627546Z","shell.execute_reply.started":"2025-05-12T09:29:52.567087Z","shell.execute_reply":"2025-05-12T09:29:52.626739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_rows = len(df_ling) # всего строк\nprocessed_rows = 0 # обработано строк\nfeatures_list = []\n\n# Обрабатываем тексты и сохраняем результаты\nfor idx, text in enumerate(df_ling['text']):\n    if pd.isna(text) or len(text.strip()) == 0:\n        processed_rows += 1\n        features_list.append({})\n        continue\n\n    try:\n        analyzer = RussianTextAnalyzer(text)\n        features = analyzer.analyze()\n        features_list.append(features)\n    except Exception as e:\n        print(f\"Ошибка при обработке текста: {e}\")\n        features_list.append({})  # Добавляем пустой словарь при ошибке\n\n    processed_rows += 1\n\n    # Выводим статистику (каждые 100 обработанных строк)\n    if processed_rows % 100 == 0:\n        percentage_complete = (processed_rows / total_rows) * 100\n        print(f\"Обработано строк: {processed_rows}/{total_rows} ({percentage_complete:.2f}%)\")\n\n\n# Создаем временный DataFrame с результатами\ntemp_df = pd.DataFrame(features_list).add_suffix('_abs')\n\n# Удаляем существующие столбцы перед объединением\nexisting_cols = df_ling.columns.intersection(temp_df.columns)\ndf_ling = df_ling.drop(columns=existing_cols)\n\n# Объединяем с исходным DataFrame\ndf_ling = pd.concat([df_ling.reset_index(drop=True),\n                    temp_df.reset_index(drop=True)], axis=1)\n\nprint(f\"Всего обработано строк: {processed_rows}\")\n\n# Сохраняем DataFrame в CSV файл \ndf_ling.to_csv('output_with_params_prozhito.csv', index=False, encoding='utf-8')\n\nprint(f\"DataFrame сохранен в файл: output_with_params_prozhito.csv\")\n\n\n","metadata":{"id":"omKaT7NZB2J5","outputId":"6ac2a5d2-e0ec-42bf-c511-f8f6442c9736","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:30:08.204033Z","iopub.execute_input":"2025-05-12T09:30:08.204662Z","iopub.status.idle":"2025-05-12T12:12:07.283476Z","shell.execute_reply.started":"2025-05-12T09:30:08.204639Z","shell.execute_reply":"2025-05-12T12:12:07.282620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_ling.to_csv('output_with_params.csv', index=False, encoding='utf-8')\n","metadata":{"id":"FzCXScStxc7v"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_ling.info()","metadata":{"id":"l9bVqF3a5L5g","executionInfo":{"status":"ok","timestamp":1745933602734,"user_tz":-180,"elapsed":58,"user":{"displayName":"Sonja Zakharova","userId":"08650122584567250304"}},"outputId":"4f980809-550e-4f3a-8252-4d9be544e592"},"outputs":[],"execution_count":null}]}